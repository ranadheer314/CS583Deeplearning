{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Version4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wulnv9Dkkxpy"
      },
      "source": [
        "import os\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import pickle as pkl\n",
        "import sys\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\") \n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4718541kxqR"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuDDH-oWkxqU"
      },
      "source": [
        "def load(opts):\n",
        "    encoder = torch.load(os.path.join(opts.load, 'encoder.pt'))\n",
        "    decoder = torch.load(os.path.join(opts.load, 'decoder.pt'))\n",
        "    idx_dict = pkl.load(open(os.path.join(opts.load, 'idx_dict.pkl'), 'rb'))\n",
        "    return encoder, decoder, idx_dict\n",
        "\n",
        "def read_lines(filename):\n",
        "    lines = open(filename).read().strip().lower().split('\\n')\n",
        "        \n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "\n",
        "def read_pairs(filename):\n",
        "    \"\"\"Reads lines that consist of two words, separated by a space.\n",
        "\n",
        "    Returns:\n",
        "        source_words: A list of the first word in each line of the file.\n",
        "        target_words: A list of the second word in each line of the file.\n",
        "    \"\"\"\n",
        "    lines = read_lines(filename)\n",
        "    source_words, target_words = [], []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            source, target = line.split()\n",
        "            source_words.append(source)\n",
        "            target_words.append(target)\n",
        "    return source_words, target_words\n",
        "\n",
        "\n",
        "\n",
        "def all_alpha_or_dash(s):\n",
        "    \"\"\"Helper function to check whether a string is alphabetic, allowing dashes '-'.\n",
        "    \"\"\"\n",
        "    return all(c.isalpha() or c == '-' for c in s)\n",
        "\n",
        "def filter_lines(lines):\n",
        "    \"\"\"Filters lines to consist of only alphabetic characters or dashes \"-\".\n",
        "    \"\"\"\n",
        "    return [line for line in lines if all_alpha_or_dash(line)]\n",
        "\n",
        "\n",
        "def load_data():\n",
        "    \"\"\"Loads (English, Pig-Latin) word pairs, and creates mappings from characters to indexes.\"\"\"\n",
        "\n",
        "    source_lines, target_lines = read_pairs('data.txt')\n",
        "\n",
        "    # Filter lines\n",
        "    source_lines = filter_lines(source_lines)\n",
        "    target_lines = filter_lines(target_lines)\n",
        "\n",
        "    all_characters = set(''.join(source_lines)) | set(''.join(target_lines))\n",
        "\n",
        "    \n",
        "    # Create a dictionary mapping each character to a unique index\n",
        "    char_to_index = { char: index for (index, char) in enumerate(sorted(list(all_characters))) }\n",
        "\n",
        "    # Add start and end tokens to the dictionary\n",
        "    start_token = len(char_to_index)\n",
        "    end_token = len(char_to_index) + 1\n",
        "    char_to_index['SOS'] = start_token\n",
        "    char_to_index['EOS'] = end_token\n",
        "\n",
        "    # Create the inverse mapping, from indexes to characters (used to decode the model's predictions)\n",
        "    index_to_char = { index: char for (char, index) in char_to_index.items() }\n",
        "\n",
        "    # Store the final size of the vocabulary\n",
        "    vocab_size = len(char_to_index)\n",
        "\n",
        "    line_pairs = list(set(zip(source_lines, target_lines)))  # Python 3\n",
        "\n",
        "    idx_dict = { 'char_to_index': char_to_index,\n",
        "                 'index_to_char': index_to_char,\n",
        "                 'start_token': start_token,\n",
        "                 'end_token': end_token }\n",
        "\n",
        "    return line_pairs, vocab_size, idx_dict\n",
        "\n",
        "def create_dict(pairs):\n",
        "    \"\"\"Creates a mapping { (source_length, target_length): [list of (source, target) pairs]\n",
        "    This is used to make batches: each batch consists of two parallel tensors, one containing\n",
        "    all source indexes and the other containing all corresponding target indexes.\n",
        "    Within a batch, all the source words are the same length, and all the target words are\n",
        "    the same length.\n",
        "    \"\"\"\n",
        "    unique_pairs = list(set(pairs))  # Find all unique (source, target) pairs\n",
        "\n",
        "    d = defaultdict(list)\n",
        "    for (s,t) in unique_pairs:\n",
        "        d[(len(s), len(t))].append((s,t))\n",
        "\n",
        "    return d\n",
        "\n",
        "\n",
        "def save_loss_plot(train_losses, val_losses, opts):\n",
        "    \"\"\"Saves a plot of the training and validation loss curves.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.plot(range(len(train_losses)), train_losses)\n",
        "    plt.plot(range(len(val_losses)), val_losses)\n",
        "    plt.title('BS={}, nhid={}'.format(opts.batch_size, opts.hidden_size), fontsize=20)\n",
        "    plt.xlabel('Epochs', fontsize=16)\n",
        "    plt.ylabel('Loss', fontsize=16)\n",
        "    plt.xticks(fontsize=14)\n",
        "    plt.yticks(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(opts.checkpoint_path, 'loss_plot.pdf'))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "\n",
        "def checkpoint(encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Saves the current encoder and decoder models, along with idx_dict, which\n",
        "    contains the char_to_index and index_to_char mappings, and the start_token\n",
        "    and end_token values.\n",
        "    \"\"\"\n",
        "    with open(os.path.join(opts.checkpoint_path, 'encoder.pt'), 'wb') as f:\n",
        "        torch.save(encoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'decoder.pt'), 'wb') as f:\n",
        "        torch.save(decoder, f)\n",
        "\n",
        "    with open(os.path.join(opts.checkpoint_path, 'idx_dict.pkl'), 'wb') as f:\n",
        "        pkl.dump(idx_dict, f)\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjZ5n9RIkxqa"
      },
      "source": [
        "### Utils for Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kZWnNcykxqc"
      },
      "source": [
        "def string_to_index_list(s, char_to_index, end_token):\n",
        "    \"\"\"Converts a sentence into a list of indexes (for each character).\n",
        "    \"\"\"\n",
        "    return [char_to_index[char] for char in s] + [end_token]  # Adds the end token to each index list\n",
        "\n",
        "\n",
        "def translate_sentence(sentence, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a sentence from English to Pig-Latin, by splitting the sentence into\n",
        "    words (whitespace-separated), running the encoder-decoder model to translate each\n",
        "    word independently, and then stitching the words back together with spaces between them.\n",
        "    \"\"\"\n",
        "    if idx_dict is None:\n",
        "      line_pairs, vocab_size, idx_dict = load_data()\n",
        "    return ' '.join([translate(word, encoder, decoder, idx_dict, opts) for word in sentence.split()])\n",
        "\n",
        "\n",
        "def translate(input_string, encoder, decoder, idx_dict, opts):\n",
        "    \"\"\"Translates a given string from English to Pig-Latin.\n",
        "    \"\"\"\n",
        "\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "    index_to_char = idx_dict['index_to_char']\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "\n",
        "    max_generated_chars = 20\n",
        "    gen_string = ''\n",
        "\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\n",
        "\n",
        "    encoder_annotations, encoder_last_hidden = encoder(indexes)\n",
        "\n",
        "    decoder_hidden = encoder_last_hidden\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\n",
        "    decoder_inputs = decoder_input\n",
        "\n",
        "    for i in range(max_generated_chars):\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\n",
        "      ni = ni[-1] \n",
        "\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\n",
        "      \n",
        "      if ni == end_token:\n",
        "          break\n",
        "      else:\n",
        "          gen_string = \"\".join(\n",
        "              [index_to_char[int(item)] \n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\n",
        "\n",
        "    return gen_string\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "    \n",
        "\n",
        "def to_var(tensor, cuda):\n",
        "    \"\"\"Wraps a Tensor in a Variable, optionally placing it on the GPU.\n",
        "\n",
        "        Arguments:\n",
        "            tensor: A Tensor object.\n",
        "            cuda: A boolean flag indicating whether to use the GPU.\n",
        "\n",
        "        Returns:\n",
        "            A Variable object, on the GPU if cuda==True.\n",
        "    \"\"\"\n",
        "    if cuda:\n",
        "        return Variable(tensor.cuda())\n",
        "    else:\n",
        "        return Variable(tensor)\n",
        "\n",
        "\n",
        "def create_dir_if_not_exists(directory):\n",
        "    \"\"\"Creates a directory if it doesn't already exist.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2sZXLhUkxqh"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG5DGFm4kxqj"
      },
      "source": [
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using TEST_SENTENCE\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(test_sentence, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n",
        "\n",
        "\n",
        "def print_data_stats(line_pairs, vocab_size, idx_dict):\n",
        "    \"\"\"Prints example word pairs, the number of data points, and the vocabulary.\"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Data Stats'.center(80))\n",
        "    print('-' * 80)\n",
        "    for pair in line_pairs[:5]:\n",
        "        print(pair)\n",
        "    print('Num unique word pairs: {}'.format(len(line_pairs)))\n",
        "    print('Vocabulary: {}'.format(idx_dict['char_to_index'].keys()))\n",
        "    print('Vocab size: {}'.format(vocab_size))\n",
        "    print('=' * 80)\n",
        "\n",
        "\n",
        "\n",
        "def print_opts(opts):\n",
        "    \"\"\"Prints the values of all command-line arguments.\n",
        "    \"\"\"\n",
        "    print('=' * 80)\n",
        "    print('Opts'.center(80))\n",
        "    print('-' * 80)\n",
        "    for key in opts.__dict__:\n",
        "        print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
        "    print('=' * 80)\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "            BS = inputs.size(0)\n",
        "\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "            decoder_hidden = encoder_hidden\n",
        "\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  \n",
        "            decoder_input = to_var(start_vector, opts.cuda)  \n",
        "            loss = 0.0\n",
        "\n",
        "            seq_len = targets.size(1) \n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  \n",
        "            \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            if optimizer:\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnJkQ1EDkxqz"
      },
      "source": [
        "### GRU cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT93bhQgkxq2"
      },
      "source": [
        "class MyGRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyGRUCell, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size, bias = False)\n",
        "        self.Wir = nn.Linear(input_size, hidden_size, bias = False)\n",
        "        self.Win = nn.Linear(input_size, hidden_size, bias = False)\n",
        "\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size)\n",
        "        \n",
        "\n",
        "\n",
        "    def forward(self, x, h_prev):\n",
        "        \"\"\"Forward pass of the GRU computation for one time step.\n",
        "\n",
        "        Arguments\n",
        "            x: batch_size x input_size\n",
        "            h_prev: batch_size x hidden_size\n",
        "\n",
        "        Returns:\n",
        "            h_new: batch_size x hidden_size\n",
        "        \"\"\"\n",
        "\n",
        "        # ------------\n",
        "        # FILL THIS IN\n",
        "        # ------------\n",
        "        z = F.sigmoid(self.Wiz(x) + self.Whz(h_prev))\n",
        "        r = F.sigmoid(self.Wir(x) + self.Whr(h_prev))\n",
        "        g = F.tanh(self.Win(x) + r * self.Whn(h_prev))\n",
        "        h_new = (1-z)*g + z*h_prev\n",
        "        return h_new\n",
        "\n",
        "        "
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_vntjITkxq3"
      },
      "source": [
        "class GRUEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, opts):\n",
        "        super(GRUEncoder, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.opts = opts\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.gru = nn.GRUCell(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Forward pass of the encoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all time steps in the sequence. (batch_size x seq_len)\n",
        "\n",
        "        Returns:\n",
        "            annotations: The hidden states computed at each step of the input sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden: The final hidden state of the encoder, for each sequence in a batch. (batch_size x hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, seq_len = inputs.size()\n",
        "        hidden = self.init_hidden(batch_size)\n",
        "\n",
        "        encoded = self.embedding(inputs)  # batch_size x seq_len x hidden_size\n",
        "        annotations = []\n",
        "\n",
        "        for i in range(seq_len):\n",
        "            x = encoded[:,i,:]  # Get the current time step, across the whole batch\n",
        "            hidden = self.gru(x, hidden)\n",
        "            annotations.append(hidden)\n",
        "\n",
        "        annotations = torch.stack(annotations, dim=1)\n",
        "        return annotations, hidden\n",
        "\n",
        "    def init_hidden(self, bs):\n",
        "        return to_var(torch.zeros(bs, self.hidden_size), self.opts.cuda)\n",
        "\n",
        "\n",
        "class NoAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(NoAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.GRUCell(input_size=hidden_size, hidden_size=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, annotations, hidden_init): \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)       \n",
        "\n",
        "        hiddens = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            x = embed[:,i,:] \n",
        "            h_prev = self.rnn(x, h_prev) \n",
        "            hiddens.append(h_prev)\n",
        "        hiddens = torch.stack(hiddens, dim=1) \n",
        "        output = self.out(hiddens)  \n",
        "        return output, None      "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDhL0Tphkxq5"
      },
      "source": [
        "### Splitting the Dataset and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhIw0rzmkxq6"
      },
      "source": [
        "def train(opts):\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\n",
        "    print_data_stats(line_pairs, vocab_size, idx_dict)\n",
        "\n",
        "    # Split the line pairs into an 80% train and 20% val split\n",
        "    num_lines = len(line_pairs)\n",
        "    num_train = int(0.8 * num_lines)\n",
        "    train_pairs, val_pairs = line_pairs[:num_train], line_pairs[num_train:]\n",
        "\n",
        "    # Group the data by the lengths of the source and target words, to form batches\n",
        "    train_dict = create_dict(train_pairs)\n",
        "    val_dict = create_dict(val_pairs)\n",
        "\n",
        "    ##########################################################################\n",
        "    ### Setup: Create Encoder, Decoder, Learning Criterion, and Optimizers ###\n",
        "    ##########################################################################\n",
        "    encoder = GRUEncoder(vocab_size=vocab_size, \n",
        "                         hidden_size=opts.hidden_size, \n",
        "                         opts=opts)\n",
        "\n",
        "    if opts.decoder_type == 'rnn':\n",
        "        decoder = NoAttentionDecoder(vocab_size=vocab_size, \n",
        "                             hidden_size=opts.hidden_size)\n",
        "    elif opts.decoder_type == 'rnn_attention':\n",
        "          decoder = RNNAttentionDecoder(vocab_size=vocab_size, \n",
        "                                      hidden_size=opts.hidden_size)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "        \n",
        "    #### setup checkpoint path\n",
        "    model_name = 'h{}-bs{}-{}'.format(opts.hidden_size, \n",
        "                                      opts.batch_size, \n",
        "                                      opts.decoder_type)\n",
        "    opts.checkpoint_path = model_name\n",
        "    create_dir_if_not_exists(opts.checkpoint_path)\n",
        "    ####\n",
        "\n",
        "    if opts.cuda:\n",
        "        encoder.cuda()\n",
        "        decoder.cuda()\n",
        "        print(\"Moved models to GPU!\")\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=opts.learning_rate)\n",
        "\n",
        "    try:\n",
        "        training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts)\n",
        "    except KeyboardInterrupt:\n",
        "        print('Exiting early from training.')\n",
        "        return encoder, decoder\n",
        "      \n",
        "    return encoder, decoder\n",
        "\n",
        "\n",
        "def compute_loss(data_dict, encoder, decoder, idx_dict, criterion, optimizer, opts):\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    losses = []\n",
        "    for key in data_dict:\n",
        "        input_strings, target_strings = zip(*data_dict[key])\n",
        "        input_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in input_strings]\n",
        "        target_tensors = [torch.LongTensor(string_to_index_list(s, char_to_index, end_token)) for s in target_strings]\n",
        "\n",
        "        num_tensors = len(input_tensors)\n",
        "        num_batches = int(np.ceil(num_tensors / float(opts.batch_size)))\n",
        "\n",
        "        for i in range(num_batches):\n",
        "            start = i * opts.batch_size\n",
        "            end = start + opts.batch_size\n",
        "            inputs = to_var(torch.stack(input_tensors[start:end]), opts.cuda)\n",
        "            targets = to_var(torch.stack(target_tensors[start:end]), opts.cuda)\n",
        "            BS = inputs.size(0)\n",
        "            encoder_annotations, encoder_hidden = encoder(inputs)\n",
        "            decoder_hidden = encoder_hidden\n",
        "            start_vector = torch.ones(BS).long().unsqueeze(1) * start_token  \n",
        "            decoder_input = to_var(start_vector, opts.cuda) \n",
        "            loss = 0.0\n",
        "            seq_len = targets.size(1) \n",
        "            decoder_inputs = torch.cat([decoder_input, targets[:, 0:-1]], dim=1)  \n",
        "            decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, encoder_hidden)\n",
        "            decoder_outputs_flatten = decoder_outputs.view(-1, decoder_outputs.size(2))\n",
        "            targets_flatten = targets.view(-1)\n",
        "            loss = criterion(decoder_outputs_flatten, targets_flatten)\n",
        "            losses.append(loss.item())\n",
        "            if optimizer:\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "                        \n",
        "    mean_loss = np.mean(losses)\n",
        "    return mean_loss\n",
        "\n",
        "\n",
        "def training_loop(train_dict, val_dict, idx_dict, encoder, decoder, criterion, optimizer, opts):\n",
        "    \"\"\"Runs the main training loop; evaluates the model on the val set every epoch.\n",
        "        * Prints training and val loss each epoch.\n",
        "        * Prints qualitative translation results each epoch using test_sentence\n",
        "        * Saves an attention map for TEST_WORD_ATTN each epoch\n",
        "\n",
        "    Arguments:\n",
        "        train_dict: The training word pairs, organized by source and target lengths.\n",
        "        val_dict: The validation word pairs, organized by source and target lengths.\n",
        "        idx_dict: Contains char-to-index and index-to-char mappings, and start & end token indexes.\n",
        "        encoder: An encoder model to produce annotations for each step of the input sequence.\n",
        "        decoder: A decoder model (with or without attention) to generate output tokens.\n",
        "        criterion: Used to compute the CrossEntropyLoss for each decoder output.\n",
        "        optimizer: Implements a step rule to update the parameters of the encoder and decoder.\n",
        "        opts: The command-line arguments.\n",
        "    \"\"\"\n",
        "\n",
        "    start_token = idx_dict['start_token']\n",
        "    end_token = idx_dict['end_token']\n",
        "    char_to_index = idx_dict['char_to_index']\n",
        "\n",
        "    loss_log = open(os.path.join(opts.checkpoint_path, 'loss_log.txt'), 'w')\n",
        "\n",
        "    best_val_loss = 1e6\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(opts.nepochs):\n",
        "\n",
        "        optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "        \n",
        "        train_loss = compute_loss(train_dict, encoder, decoder, idx_dict, criterion, optimizer, opts)\n",
        "        val_loss = compute_loss(val_dict, encoder, decoder, idx_dict, criterion, None, opts)\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            checkpoint(encoder, decoder, idx_dict, opts)\n",
        "\n",
        "        gen_string = translate_sentence(test_sentence, encoder, decoder, idx_dict, opts)\n",
        "        print(\"Epoch: {:3d} | Train loss: {:.3f} | Val loss: {:.3f} | Gen: {:20s}\".format(epoch, train_loss, val_loss, gen_string))\n",
        "\n",
        "        loss_log.write('{} {} {}\\n'.format(epoch, train_loss, val_loss))\n",
        "        loss_log.flush()\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        save_loss_plot(train_losses, val_losses, opts)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvgy-GRA1V_Q"
      },
      "source": [
        "### RNN Model Implemenetation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssh92Cfbkxq7",
        "outputId": "8ccd80ea-d3c6-4f70-9f65-31e871bff1f5"
      },
      "source": [
        "test_sentence=\"here hang a pearl in every cowslip ear\"\n",
        "torch.manual_seed(1)\n",
        "args = AttrDict()\n",
        "args_dict = {'cuda':True, 'nepochs':100, 'checkpoint_dir':\"checkpoints\", 'learning_rate':0.005, 'lr_decay':0.99,'batch_size':64, 'hidden_size':20, 'decoder_type': 'rnn'}\n",
        "args.update(args_dict)\n",
        "print_opts(args)\n",
        "rnn_encoder, rnn_decoder = train(args)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn                                    \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('sober', 'obersay')\n",
            "('grey', 'eygray')\n",
            "('winter', 'interway')\n",
            "('downs', 'ownsday')\n",
            "('inforced', 'inforcedway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.327 | Val loss: 2.058 | Gen: ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ongay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ongay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay\n",
            "Epoch:   1 | Train loss: 1.920 | Val loss: 1.929 | Gen: ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ingsay-ay-ay-ay-ay-a away-ay-ay-ay-ay-ay- oongway ay-ay-ay-ay-ay-ay-ay\n",
            "Epoch:   2 | Train loss: 1.786 | Val loss: 1.853 | Gen: away-ay-ay-ay-ay-ay- ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay ingway ay-ay-ay-ay-ay-ay-ay oongsay-ay-ay-ay-ay- away-ay-ay-ay-ay-ay-\n",
            "Epoch:   3 | Train loss: 1.684 | Val loss: 1.777 | Gen: away-ay-ay-ay-ay-ay- ay-ay-ay-ay-ay-ay-ay ay-ay-ay-ay-ay-ay-ay away-ay-ay-ay-ay-ay- iway-ay-ay-ay-ay-ay- antingway oongsay-ingway away-ay-ay-ay-ay-ay-\n",
            "Epoch:   4 | Train loss: 1.612 | Val loss: 1.714 | Gen: away-ay-away-ay-ay-a away-ay-ay-ay-ay-ay- away-ay-ay-ay-ayway away-ay-ay-away-ay-a iway-ay-ay-ay-ay-ay- antay-ay-away-ay-awa oongsay-ingway away-ay-ay-away-ay-a\n",
            "Epoch:   5 | Train loss: 1.550 | Val loss: 1.672 | Gen: away-ay-ay-ay-ay-ay- away-ay-ay-ay-ay-ay- ay-ay away-ay-ay-ay-ay-ay- iway eartay-ay-ay-atedway oongsay-ingway away-ay-ay-ay-ay\n",
            "Epoch:   6 | Train loss: 1.496 | Val loss: 1.662 | Gen: earay-ay-ay-ay-ayday away-ay-ay-ay-ay-ay away away-ay-ay-ay-ay-ay- iway-yway-ay-ay-ay-a eartay-ay-ay-ay-ay-a ongsay-ingway away-ay-ay-ay\n",
            "Epoch:   7 | Train loss: 1.460 | Val loss: 1.620 | Gen: eanway-ay-ayday away-ay-ay-ay-ay-ay- away away-ay-ay-ayday iway-ingway eantay-ay-astay-ay-a ongsay-istay-ingsay away-ayday\n",
            "Epoch:   8 | Train loss: 1.433 | Val loss: 1.632 | Gen: elay-away-away-away- away-ay-ayday away-ayday eanway-ay-ayday iway elleway-ay-away-away ongsay-ingsay away-away-ayday\n",
            "Epoch:   9 | Train loss: 1.429 | Val loss: 1.578 | Gen: eay-away away away away-ay-ayday iway eay-ay-estay ingssay away\n",
            "Epoch:  10 | Train loss: 1.369 | Val loss: 1.539 | Gen: eancedway away-ayday away alay-away-ayday ingway eancencay ongsingway away-ayday\n",
            "Epoch:  11 | Train loss: 1.333 | Val loss: 1.524 | Gen: eanway away-ay away alay ingway eanceway ingsspay away\n",
            "Epoch:  12 | Train loss: 1.298 | Val loss: 1.509 | Gen: earay allyway away allway-ayday ingway earlyway ingsssay away-ayday\n",
            "Epoch:  13 | Train loss: 1.277 | Val loss: 1.493 | Gen: earway-ayday anway-ayday away earway-ayday ingway eeleway-ayday ontingsay away-ayday\n",
            "Epoch:  14 | Train loss: 1.262 | Val loss: 1.472 | Gen: earay away-ay away earway-ayday ingway eelay oningway away\n",
            "Epoch:  15 | Train loss: 1.233 | Val loss: 1.449 | Gen: earay anway-ayday away earway-ayday ingway eeringway oningsay away-ayday\n",
            "Epoch:  16 | Train loss: 1.207 | Val loss: 1.474 | Gen: earay anway-yeay away allway inway eeryway ingloulway away\n",
            "Epoch:  17 | Train loss: 1.181 | Val loss: 1.440 | Gen: earay anway away earway ingway eeringway oningsay away\n",
            "Epoch:  18 | Train loss: 1.169 | Val loss: 1.510 | Gen: earay anway-ayday away allay inway eerway inkonway-ay-ay away-ayday\n",
            "Epoch:  19 | Train loss: 1.162 | Val loss: 1.416 | Gen: earyway angay away earlay ingway eeringway ointingway away-ayday\n",
            "Epoch:  20 | Train loss: 1.135 | Val loss: 1.389 | Gen: earway anway away earway-ayday inway eerinedway oinway-ingway earay\n",
            "Epoch:  21 | Train loss: 1.114 | Val loss: 1.391 | Gen: erray angay away earway-yearay inway emerway ointingway earay\n",
            "Epoch:  22 | Train loss: 1.094 | Val loss: 1.383 | Gen: earyway angay away earway-inayday ingway eeringway insounway-ingway away-ayday\n",
            "Epoch:  23 | Train loss: 1.072 | Val loss: 1.337 | Gen: earway angay away earway-ayday inway eeringway inounsingway earay\n",
            "Epoch:  24 | Train loss: 1.056 | Val loss: 1.349 | Gen: earray angay away earlyway ingway eerinway-ayday oinwicay earay\n",
            "Epoch:  25 | Train loss: 1.045 | Val loss: 1.388 | Gen: earway angay away earway-ayday inway eerinedway oinway-ay-asway earay\n",
            "Epoch:  26 | Train loss: 1.034 | Val loss: 1.396 | Gen: eeray angray away earlay ingway eerindway ointingway earay\n",
            "Epoch:  27 | Train loss: 1.029 | Val loss: 1.363 | Gen: earway anglay away ealway inway eminway oinwingway away-ayday\n",
            "Epoch:  28 | Train loss: 1.020 | Val loss: 1.335 | Gen: ecay angay away ealway ingway eeringway onwicesway eay-ayday\n",
            "Epoch:  29 | Train loss: 1.016 | Val loss: 1.376 | Gen: earray angray away earlay ingway eerinesway oincessway eay-ayday\n",
            "Epoch:  30 | Train loss: 0.989 | Val loss: 1.340 | Gen: earway angray away ealway ingway eeringway onwicesway eayday\n",
            "Epoch:  31 | Train loss: 0.967 | Val loss: 1.299 | Gen: earway angay away ealway ingway eerinway oinwingway eay-ayday\n",
            "Epoch:  32 | Train loss: 0.955 | Val loss: 1.302 | Gen: earway angay away ealway ingway eerinway ointeway eay-ayday\n",
            "Epoch:  33 | Train loss: 0.941 | Val loss: 1.285 | Gen: earway angay away ealway inway eeriway ovelyway eayday\n",
            "Epoch:  34 | Train loss: 0.938 | Val loss: 1.292 | Gen: eay-ayday anglay away ealway ingway eevingway oniblishay eay\n",
            "Epoch:  35 | Train loss: 0.974 | Val loss: 1.397 | Gen: earray angay-oundedway away eallway ingway exay-aysday ounway-ulway eay-ayday\n",
            "Epoch:  36 | Train loss: 0.966 | Val loss: 1.290 | Gen: eay-ayday anglay away ealway inway eeriway ovenway away\n",
            "Epoch:  37 | Train loss: 0.928 | Val loss: 1.260 | Gen: earway angay-okedway away ealway ingway everway oinsensway eay-ayday\n",
            "Epoch:  38 | Train loss: 0.911 | Val loss: 1.239 | Gen: earway angay away ealway inway eevingway ovinenay eay-ayday\n",
            "Epoch:  39 | Train loss: 0.899 | Val loss: 1.344 | Gen: earway angay away ealway inway eeveway ovelway earay\n",
            "Epoch:  40 | Train loss: 0.901 | Val loss: 1.342 | Gen: earway angay away ealesway inway everay ovenway eay-ayday\n",
            "Epoch:  41 | Train loss: 0.895 | Val loss: 1.271 | Gen: eay-yeay aghedway away ealway ingway everay ovenway-awlay eay\n",
            "Epoch:  42 | Train loss: 0.889 | Val loss: 1.212 | Gen: earway aghedway away ealway ingway everway ovinepray eay-ayday\n",
            "Epoch:  43 | Train loss: 0.873 | Val loss: 1.244 | Gen: earway angay away ealway inway everway oveningway eay\n",
            "Epoch:  44 | Train loss: 0.861 | Val loss: 1.235 | Gen: earway anglay away ealway inway everway ovensingway eay-ayday\n",
            "Epoch:  45 | Train loss: 0.856 | Val loss: 1.386 | Gen: earway aghay-ybay away-ayday ealway inway everway ovensingway earway\n",
            "Epoch:  46 | Train loss: 0.866 | Val loss: 1.232 | Gen: ecay angway away ealway inway everway ovenionway eay\n",
            "Epoch:  47 | Train loss: 0.860 | Val loss: 1.273 | Gen: eray-ayday angway-yeadway away ealway inway everway ovensingway earway\n",
            "Epoch:  48 | Train loss: 0.853 | Val loss: 1.203 | Gen: earway anglay away ealeway inway everway ovenway-oungway earway\n",
            "Epoch:  49 | Train loss: 0.848 | Val loss: 1.218 | Gen: earway angingway away ealway inway everway ovinway-oanway-ayday eay\n",
            "Epoch:  50 | Train loss: 0.847 | Val loss: 1.181 | Gen: earway anglay away ealway inway everway ovensway eay-yeayray\n",
            "Epoch:  51 | Train loss: 0.826 | Val loss: 1.190 | Gen: ecay anglay awaydaydaydaydaydayd eallway inway everway ovensionway earway\n",
            "Epoch:  52 | Train loss: 0.823 | Val loss: 1.127 | Gen: earway anglay away eallysay inway everway ovinepray earway\n",
            "Epoch:  53 | Train loss: 0.803 | Val loss: 1.117 | Gen: eray-yearay anglay away ealway inway everway ovensingway earway\n",
            "Epoch:  54 | Train loss: 0.795 | Val loss: 1.133 | Gen: eray-ourway anglay away ealway inway everway ovinway-oantay earway\n",
            "Epoch:  55 | Train loss: 0.785 | Val loss: 1.112 | Gen: earway anglay away eallyway inway everway ovensingway earway\n",
            "Epoch:  56 | Train loss: 0.780 | Val loss: 1.125 | Gen: earway angway away ealeslay inway everway ovensingway earway\n",
            "Epoch:  57 | Train loss: 0.780 | Val loss: 1.185 | Gen: eray-yearay anglay away eallyway inway everway ovensingway earway\n",
            "Epoch:  58 | Train loss: 0.784 | Val loss: 1.142 | Gen: earway angway away ealehay inway everway insoinway earway\n",
            "Epoch:  59 | Train loss: 0.778 | Val loss: 1.142 | Gen: eray-yeadway anglay away ealeslay inway everway ovensingway earway\n",
            "Epoch:  60 | Train loss: 0.764 | Val loss: 1.134 | Gen: earway anglay away ealway inway everway ovensinay eayway\n",
            "Epoch:  61 | Train loss: 0.773 | Val loss: 1.242 | Gen: eray-yeaay angway-olway away ealeway inway everway ovensinay earway\n",
            "Epoch:  62 | Train loss: 0.792 | Val loss: 1.147 | Gen: eray-ousedway anglay away ealespray inway everway ovenspay earway\n",
            "Epoch:  63 | Train loss: 0.764 | Val loss: 1.090 | Gen: earway anglay away ealway inway everway oveppay earway\n",
            "Epoch:  64 | Train loss: 0.745 | Val loss: 1.114 | Gen: eray-yearway angingway away eallyway inway everway ovensinay earway\n",
            "Epoch:  65 | Train loss: 0.740 | Val loss: 1.149 | Gen: earway anglay away ealeslay inway everway ovensingway earway\n",
            "Epoch:  66 | Train loss: 0.741 | Val loss: 1.103 | Gen: eray-yearay angway away ealepay inway everway ovensingway earway\n",
            "Epoch:  67 | Train loss: 0.737 | Val loss: 1.102 | Gen: eray-ousway anglay away ealway inway everway oveppray earway\n",
            "Epoch:  68 | Train loss: 0.724 | Val loss: 1.092 | Gen: eray-yearay angway away eallyway inway everway ovensinay earway\n",
            "Epoch:  69 | Train loss: 0.715 | Val loss: 1.102 | Gen: eray-yearay anglay away ealepray inway everway oveppray earway\n",
            "Epoch:  70 | Train loss: 0.710 | Val loss: 1.157 | Gen: earway angway away ealeslay inway everway ovensinay earway\n",
            "Epoch:  71 | Train loss: 0.712 | Val loss: 1.162 | Gen: eray-yearay anglay away ealespay inway everway ovenspay earway\n",
            "Epoch:  72 | Train loss: 0.719 | Val loss: 1.200 | Gen: earway angway away ealway inway everway oveppray aray\n",
            "Epoch:  73 | Train loss: 0.725 | Val loss: 1.116 | Gen: earway angway away ealeway inway everway ovensinway earway\n",
            "Epoch:  74 | Train loss: 0.721 | Val loss: 1.227 | Gen: exay-ybay anghay away ealespay inway-ayday eversway ovensionway earay\n",
            "Epoch:  75 | Train loss: 0.741 | Val loss: 1.147 | Gen: earhay angingway away ealouway inway everway ovensingway array\n",
            "Epoch:  76 | Train loss: 0.736 | Val loss: 1.143 | Gen: earway angway away eallyway inway everway ovensinway eayway\n",
            "Epoch:  77 | Train loss: 0.705 | Val loss: 1.062 | Gen: earway anghay away ealepray inway everway ovensinway earway\n",
            "Epoch:  78 | Train loss: 0.688 | Val loss: 1.066 | Gen: eray-ybay anglay away eallyway inway everway ovinsoweway earway\n",
            "Epoch:  79 | Train loss: 0.680 | Val loss: 1.049 | Gen: eray-yeay anglay away ealepay inway everway oveppray earway\n",
            "Epoch:  80 | Train loss: 0.672 | Val loss: 1.061 | Gen: eray-ybay anghay away ealepay inway everway oveppray earway\n",
            "Epoch:  81 | Train loss: 0.670 | Val loss: 1.089 | Gen: eray-yearay angway away ealepray inway eversway ovinseway earway\n",
            "Epoch:  82 | Train loss: 0.670 | Val loss: 1.073 | Gen: erehay anghay away ealepay inway eeveway ovensinway earway\n",
            "Epoch:  83 | Train loss: 0.672 | Val loss: 1.046 | Gen: eray-yeaay angway away ealepay inway everway ovinsway earway\n",
            "Epoch:  84 | Train loss: 0.672 | Val loss: 1.123 | Gen: eray-ybay anghay away ealingway inway everway ovensikyway earway\n",
            "Epoch:  85 | Train loss: 0.689 | Val loss: 1.235 | Gen: earway anginway away ealreway inway everway ovinshay earway\n",
            "Epoch:  86 | Train loss: 0.702 | Val loss: 1.093 | Gen: earway angway away earlyjay inway everway ovonspay aray\n",
            "Epoch:  87 | Train loss: 0.682 | Val loss: 1.104 | Gen: earway angway-ukeway away ealepay inway everway ovensinway earway\n",
            "Epoch:  88 | Train loss: 0.666 | Val loss: 1.046 | Gen: eray-ybay anglay away ealingway inway everway ovinshay earway\n",
            "Epoch:  89 | Train loss: 0.648 | Val loss: 1.051 | Gen: eray-ybay angway away ealingway inway everway ovinsway earway\n",
            "Epoch:  90 | Train loss: 0.640 | Val loss: 1.032 | Gen: eray-ybay angway-ounedway away eallyway inway everway ovinsway earway\n",
            "Epoch:  91 | Train loss: 0.638 | Val loss: 1.040 | Gen: eray-ybay anghay away ealepray inway everway ovinsway earway\n",
            "Epoch:  92 | Train loss: 0.635 | Val loss: 1.033 | Gen: earway angway away eallysay inway everway ovinsway earway\n",
            "Epoch:  93 | Train loss: 0.639 | Val loss: 1.056 | Gen: eray-ayday anghay away ealepray inway everway ovinslyway earway\n",
            "Epoch:  94 | Train loss: 0.640 | Val loss: 1.105 | Gen: eray-ybay anghay away ealryway inway everway ovinsway earway\n",
            "Epoch:  95 | Train loss: 0.653 | Val loss: 1.102 | Gen: eray-yeay anghay away ealingway inway everway ovonsingway earway\n",
            "Epoch:  96 | Train loss: 0.655 | Val loss: 1.077 | Gen: erehay angway away ealepray inway everway ovinseway earway\n",
            "Epoch:  97 | Train loss: 0.656 | Val loss: 1.031 | Gen: eray-ybay anghay away ealepay inway everway ovinsway earway\n",
            "Epoch:  98 | Train loss: 0.657 | Val loss: 1.122 | Gen: earway angway-oldedway away ealingway inway everway ovinshblay earway\n",
            "Epoch:  99 | Train loss: 0.648 | Val loss: 1.024 | Gen: erehay anghay away ealepray inway everway ovinshay earway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So7M0P5x4qVN",
        "outputId": "14a23f9c-48b5-4393-ac58-5c5060e9d4b3"
      },
      "source": [
        "test_sentence = 'here hang a pearl in every asdbjhbahsjfkd edwqhhjkasdfaghjs adjhas dasjhgfaskjh'\r\n",
        "long_sentence=\"here hang a pearl in every cowslip ear\"\r\n",
        "translated_RNN = translate_sentence(test_sentence, rnn_encoder, rnn_decoder, None, args)\r\n",
        "long_translated_RNN = translate_sentence(long_sentence, rnn_encoder, rnn_decoder, None, args)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(test_sentence, translated_RNN))\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(long_sentence, long_translated_RNN))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\there hang a pearl in every asdbjhbahsjfkd edwqhhjkasdfaghjs adjhas dasjhgfaskjh \n",
            "translated:\terehay anghay away ealepray inway everway asksay-ostingway undhusay-ationsway addjay-ousfay askuathlyjay\n",
            "source:\t\there hang a pearl in every cowslip ear \n",
            "translated:\terehay anghay away ealepray inway everway ovinshay earway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiVwJaqW1HKZ"
      },
      "source": [
        "### Model for Long sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCvEgnc_kxq_",
        "outputId": "c0265c31-ab77-42aa-add6-c5e620490cac"
      },
      "source": [
        "long_sentence = 'here hang a pearl in every cowslip ear'\n",
        "torch.manual_seed(1)\n",
        "args_long = AttrDict()\n",
        " ## Changed Hidden Size from 20 to 40 to make it long on long sentences\n",
        "args_dict_long = {'cuda':True, 'nepochs':100, 'checkpoint_dir':\"checkpoints\",\n",
        "                  'learning_rate':0.005, 'lr_decay':0.99,'batch_size':64, 'hidden_size':40,\n",
        "                  'decoder_type': 'rnn'}\n",
        "args_long.update(args_dict_long)\n",
        "print_opts(args_long)\n",
        "rnn_encoder_long, rnn_decoder_long = train(args_long)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 40                                     \n",
            "                           decoder_type: rnn                                    \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('sober', 'obersay')\n",
            "('grey', 'eygray')\n",
            "('winter', 'interway')\n",
            "('downs', 'ownsday')\n",
            "('inforced', 'inforcedway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.090 | Val loss: 1.857 | Gen: esay-ay ay ay esestay ingshay elesay away esessay away inseday\n",
            "Epoch:   1 | Train loss: 1.678 | Val loss: 1.741 | Gen: eredway-ayday away away eatedway ingway esedway-ayday away-ayday elestay-ayday away-ayday away-ayday\n",
            "Epoch:   2 | Train loss: 1.484 | Val loss: 1.585 | Gen: eredway anday away earay-ayday ingway eray-ayday-ayday away-ayday eay-ayday-ayday away-ayday away-ayday\n",
            "Epoch:   3 | Train loss: 1.326 | Val loss: 1.499 | Gen: eredway andway away earayday ingway earedway away-ayday eadway away-ayday away-ayday\n",
            "Epoch:   4 | Train loss: 1.208 | Val loss: 1.429 | Gen: eredway andway away earay-ayday ingway earay-ayday away-ayday-ayday eadray-oway-ayday away-ayday awlay-oway-ayday\n",
            "Epoch:   5 | Train loss: 1.125 | Val loss: 1.370 | Gen: eredway andway away earway ingway eray-ayday awlay eadway away-ayday awlay\n",
            "Epoch:   6 | Train loss: 1.039 | Val loss: 1.312 | Gen: eredway andway away eaday-ayday ingway earay-ayday away-orway-ayday eackway-oway-ayday away-ayday ay-outhay-oway-ayday\n",
            "Epoch:   7 | Train loss: 0.970 | Val loss: 1.224 | Gen: eeredway andway away eardway inway eredway awlay-orway-ayday effay-owlay away-ayday awlay\n",
            "Epoch:   8 | Train loss: 0.908 | Val loss: 1.190 | Gen: eredway anghay-ayday away-ayday earday-ayday ingway earedway awlray ecodway-ybay away-ybay asfullay\n",
            "Epoch:   9 | Train loss: 0.860 | Val loss: 1.228 | Gen: eeredway angway away-ayday eadbay-away-ayday ingway eederay away-orway-ayday ecowlay away-ayday acthoughtedway\n",
            "Epoch:  10 | Train loss: 0.830 | Val loss: 1.182 | Gen: eeredway anghay-yeay away earday-ayday inway eederay awlay-ourshay ecolway-ayday away-ayday asfulay-andway-ayday\n",
            "Epoch:  11 | Train loss: 0.790 | Val loss: 1.237 | Gen: eeredway angway-ayday away-ayday earday inway eederay awdray-awlay edcay-owray-awlay awday-ybay awlay-owray-awlay\n",
            "Epoch:  12 | Train loss: 0.765 | Val loss: 1.165 | Gen: ererway anghay-yedway-oway-o away-ayday eardshay inway eedway-yedway-yedway aspray-oway-owsay efsospray-oway-inway awday-ybay asfulay-orfay-othing\n",
            "Epoch:  13 | Train loss: 0.728 | Val loss: 1.114 | Gen: eredway-yedway agghay away eardbay inway emerway-ayday awdbray-oway-oway edpray-oway-oway away-ay-ayday asfulay\n",
            "Epoch:  14 | Train loss: 0.693 | Val loss: 1.047 | Gen: eerway anghay-awlay away-ayday eadblay-ybay inway eedway-ybay awdray-owsay eplospray awlay asfulay-ofray\n",
            "Epoch:  15 | Train loss: 0.659 | Val loss: 1.063 | Gen: eredway angbay away easdray- inway everway-oway-oway-es awdray-owsay edpray-oway-oway-owe awlay asfulray\n",
            "Epoch:  16 | Train loss: 0.636 | Val loss: 1.189 | Gen: eredway-yedway-ybay angbay away-ayday earlay inway everway-ayday awdray-owshpay edpray-owshedway away-oughtway-owsay afsquay-orfay-owestw\n",
            "Epoch:  17 | Train loss: 0.631 | Val loss: 1.081 | Gen: ererway aggnay away eardbray inway everway-ybay awrydowray edpowray-owsay awlhay asfuray-ofgray\n",
            "Epoch:  18 | Train loss: 0.605 | Val loss: 1.066 | Gen: ererway anghay away-ayday earlay-axsay inway everway-ayday-ybay awdrshpay-ooshay-awl edpray-ooshesway-owa awdhay-ybay asfulgray-owsay-axta\n",
            "Epoch:  19 | Train loss: 0.576 | Val loss: 1.034 | Gen: ererway angway away-ayday eardbay inway everway-ayday awdary-oughtway-owsa edpray-owshay-oweswa away-owray afsuray-ofway-ofway\n",
            "Epoch:  20 | Train loss: 0.562 | Val loss: 0.978 | Gen: ererway agghay away eardblay inway everyway asoodgay-ybay epdossay-owray-owsay awlay aspoughtfay-ofway\n",
            "Epoch:  21 | Train loss: 0.542 | Val loss: 0.990 | Gen: ererway anghay away eardblay inway everray ardhoaghshay edpowfay awhay-owway-owsay asfughlay-ofway\n",
            "Epoch:  22 | Train loss: 0.537 | Val loss: 1.033 | Gen: ererway anghay away-ybay eardblay inway everway-ybay ashorsday-ybay edpray-owsay awdurshay assusforspay\n",
            "Epoch:  23 | Train loss: 0.531 | Val loss: 0.987 | Gen: ererway angway away eardblay inway everray-ayday apporsday-owray-owsa epdossay-oghenstay awhay-owsay-awlay afsucompray-awlay\n",
            "Epoch:  24 | Train loss: 0.500 | Val loss: 0.910 | Gen: erehay aghnay away eardbay inway everway-ayday aporsday-oway-oway-o edposhay-away-ashion away-owsay-ayday afsucompray\n",
            "Epoch:  25 | Train loss: 0.479 | Val loss: 0.967 | Gen: erehay angay away-ayday earlay inway everway-ayday aposdargay-aysday edpray-ofway-owshay away-owsay-ayday asfoughsay-away-aysd\n",
            "Epoch:  26 | Train loss: 0.465 | Val loss: 0.911 | Gen: erehhay anghay away earlay inway everway-ovay-ayday apdossray-owhay-osha epdosshpay awhay-aysday afsuplofay\n",
            "Epoch:  27 | Train loss: 0.468 | Val loss: 1.026 | Gen: erehay anghay away eardbay inway everyway apsorsday-aysday epdssosssay-oofay-ot away-odhay-aysway asfulascray\n",
            "Epoch:  28 | Train loss: 0.479 | Val loss: 0.943 | Gen: erehhay angway away earlay inway everyway apoodgay-orfhay-ossa epcookshsay-owsay-aw awhay-ayday afsoucolflay\n",
            "Epoch:  29 | Train loss: 0.454 | Val loss: 0.978 | Gen: erehhay agnay away eardblay inway everyway aporsdoagshay epdosshposhay-ofway awhosday afsoouclypay\n",
            "Epoch:  30 | Train loss: 0.463 | Val loss: 0.997 | Gen: erehhay-aysday angway-ayday away-ayday earlay inway everway-ayday apporsday-aysday epdssospay-ofway away-owshday asfulay-ofspay\n",
            "Epoch:  31 | Train loss: 0.461 | Val loss: 0.904 | Gen: erehay angway away eardblay inway everyway aposarkshay epdosshay-orfay awhay-owsay-aysday asfulagforway-ofway\n",
            "Epoch:  32 | Train loss: 0.428 | Val loss: 0.925 | Gen: erehhay anghay away eardbay inway everray apoodgsay-aysday epdssospay-oshessspa awhoussway afsuploway-ayday\n",
            "Epoch:  33 | Train loss: 0.417 | Val loss: 0.944 | Gen: erehay anghay away earlay inway everyway apomsorsway-ayday epflay-ooshpay awhousdway afspoogay-ayday\n",
            "Epoch:  34 | Train loss: 0.403 | Val loss: 0.860 | Gen: erehay anghay away eardblay inway everray aposdarshay epfalsopray awhousdway afsfulpray\n",
            "Epoch:  35 | Train loss: 0.377 | Val loss: 0.880 | Gen: erehehay anghay away earlay inway everyway apoodgay-aysday eppofsday awhousdway afsougolfay\n",
            "Epoch:  36 | Train loss: 0.373 | Val loss: 0.865 | Gen: erehay anghay away eardblay inway everyway aporsday-aysway epdasospray awhay-asssway afsoughoudday\n",
            "Epoch:  37 | Train loss: 0.361 | Val loss: 0.852 | Gen: erehay angway away earlay inway everyway aporsday-aysday edpscoppray awhay-owsay-aysday afsoughtfulway\n",
            "Epoch:  38 | Train loss: 0.362 | Val loss: 1.017 | Gen: erehay agnay away earlgay inway everway aporfakshay efplofsay awhay-away-aysday afflsay-oogray\n",
            "Epoch:  39 | Train loss: 0.373 | Val loss: 0.899 | Gen: erehay anghay away eardblay inway everyway aporfosday-aysday edpscopgray awhay-aysday afsooughtflay\n",
            "Epoch:  40 | Train loss: 0.353 | Val loss: 0.940 | Gen: erehiblyway anghay away earlay inway everyway apordashingway epdoshpay-osay-owhay awhay-oway-ayday afsoudgay-oadgray\n",
            "Epoch:  41 | Train loss: 0.358 | Val loss: 0.945 | Gen: erehay anghay away earplay inway everway apoodsay-aysay-ayday epdosshay-ashionpay awhay-aysday akshosay-axtpay\n",
            "Epoch:  42 | Train loss: 0.372 | Val loss: 0.961 | Gen: eerhay anghay away eardblay inway everday apporsday-away-aysda edpchposshay-osay-aw awhay-aysday afspoodgay-ayday-ous\n",
            "Epoch:  43 | Train loss: 0.367 | Val loss: 0.903 | Gen: erehay anghay away eardblay inway everyway apomorshay-ayday epcokshay-away-aysda awhoudsway afsoppomnay-away-ayd\n",
            "Epoch:  44 | Train loss: 0.341 | Val loss: 0.850 | Gen: eerhay anghay away earlay inway everyway aporsdablyway epdoshplay awhay-aydoway afsoughtmay-owhay-ay\n",
            "Epoch:  45 | Train loss: 0.319 | Val loss: 0.796 | Gen: erehay anghay away earlay inway everyway apordosshay-ashbay edpscoppay awhay-owsmay afsoudgolfhay\n",
            "Epoch:  46 | Train loss: 0.303 | Val loss: 0.826 | Gen: erehay angway away earlay inway everyway apordshay-asousray edpscoaghtay-ayday awhay-aysday afsoughtloflay\n",
            "Epoch:  47 | Train loss: 0.298 | Val loss: 0.820 | Gen: erehay anghay away eardblay inway everywray apordshpay-oussday epdoshpay-oushay awhay-owsmay ayskposgray\n",
            "Epoch:  48 | Train loss: 0.305 | Val loss: 1.033 | Gen: erehay anghay away earlay inway everyway aporfsway-oompay edpschpay awhossmay afsomporfosway\n",
            "Epoch:  49 | Train loss: 0.375 | Val loss: 1.055 | Gen: eerhay anghay away eardblay inway everyway adoghgssay-aysday edpsoablyway awhay-ossay-awlay askspongspay\n",
            "Epoch:  50 | Train loss: 0.406 | Val loss: 0.934 | Gen: eerhay aghnay away eapray-away-ayday inway everyway aposprowdsway eppodshsway-ayday awhossdway apfsouchtpray\n",
            "Epoch:  51 | Train loss: 0.388 | Val loss: 1.140 | Gen: eerhay anghay-ayday away-ayday earlay-ayday inway-ayday everway-iarway-ayday awdoarghsway-owfay epdsostay-ofway-osay away-owssway-owsay-a afsfoughtdfay\n",
            "Epoch:  52 | Train loss: 0.372 | Val loss: 0.841 | Gen: erehay anghay away eardblay inway everyway aposkway-orfhay epdosshay-ashiongray awhay-ashcay afshpousddsnay\n",
            "Epoch:  53 | Train loss: 0.321 | Val loss: 0.876 | Gen: erehay anghay away earlay inway everyway aposdarway-owsay epdoshanshday awhay-aysday afsfoughtlay-away-ay\n",
            "Epoch:  54 | Train loss: 0.291 | Val loss: 0.766 | Gen: erehay anghay away earlay inway everyway apordosshay-asousbay epdosshpay-oasedfay awhay-owssway askfosagpay\n",
            "Epoch:  55 | Train loss: 0.271 | Val loss: 0.788 | Gen: erehay anghay away earlay inway everyway aposdasbray epdossagonsway awhay-asoulway afspoldway-owway\n",
            "Epoch:  56 | Train loss: 0.260 | Val loss: 0.768 | Gen: erehay anghay away earlay inway everyway aposdadbray epdosshangsay awhaydosay afspomspray\n",
            "Epoch:  57 | Train loss: 0.251 | Val loss: 0.779 | Gen: erehay anghay away earlay inway everyway aposdasrodhay epdoshpay-osshanshda awhay-owsmay afspokshay-asonday\n",
            "Epoch:  58 | Train loss: 0.246 | Val loss: 0.778 | Gen: erehay anghay away earlay inway everyway aposdaswray epdosshpay-oathingwa awhosakway asfolfspay\n",
            "Epoch:  59 | Train loss: 0.241 | Val loss: 0.791 | Gen: erehay anghay away earlay inway everyway aposdassrypray epplosssschay awhay-owssway asfolspay-oompray\n",
            "Epoch:  60 | Train loss: 0.245 | Val loss: 0.908 | Gen: eerhiblyway anghay away earlay inway everyway apdosprachray epdospay-ashionday awhay-owsmay afspoksay-ablay-away\n",
            "Epoch:  61 | Train loss: 0.261 | Val loss: 0.819 | Gen: erehay anghay away earlay inway everyway aposdorssay-oardbay epdosspasspray awhay-usmhay-acoussw asfulpray-ooghsway\n",
            "Epoch:  62 | Train loss: 0.248 | Val loss: 0.881 | Gen: erehay anghay away earlay inway everyway aposdasbray epdoashingsspray awhay-oussday asfulpray-oofhpay\n",
            "Epoch:  63 | Train loss: 0.245 | Val loss: 0.817 | Gen: erehay anhbay away earlay inway everyway adshorpay-abstay-oac epdossshpay-oasfay awhosakway afsoupskfray\n",
            "Epoch:  64 | Train loss: 0.239 | Val loss: 0.846 | Gen: erehay anghay away earlay inway everyway adspray-oofay epdosshplay awhay-oofsway asfulprofspay\n",
            "Epoch:  65 | Train loss: 0.237 | Val loss: 0.849 | Gen: erehay anghay away earlgray inway everyway aposdosspray-osay-aw epdosshpay-osheray awhay-asoysway appossfrysdway\n",
            "Epoch:  66 | Train loss: 0.235 | Val loss: 0.793 | Gen: eerhay angsway away earlay inway everyway adspray-oofhay edpsagonsway awhay-usmschay afsflouspray\n",
            "Epoch:  67 | Train loss: 0.229 | Val loss: 0.812 | Gen: erehay anghay away earlay inway everyway apdossspray-oofay epdosshpay-oasmingwa awhay-oassmay askfusporfay\n",
            "Epoch:  68 | Train loss: 0.217 | Val loss: 0.845 | Gen: erehay anghay away earlgay inway everyway adsproasway edpsoasbosway awhossway askfsay-ompray\n",
            "Epoch:  69 | Train loss: 0.219 | Val loss: 0.826 | Gen: eerhay anghay away earlay inway everyway apdossproshay epdschathingbay awhay-aysday askfusporgay-ayday\n",
            "Epoch:  70 | Train loss: 0.218 | Val loss: 0.858 | Gen: eerhay anghay away earllay inway everyway adsporshay edpschposay-andway-a awhay-aysday askfsay-opray\n",
            "Epoch:  71 | Train loss: 0.216 | Val loss: 0.879 | Gen: eerhay anghay away earlay inway everyway apporsdsssay epplosssssingcay awhay-aysday asfolspay-ookray\n",
            "Epoch:  72 | Train loss: 0.227 | Val loss: 0.884 | Gen: eerhay anhgay away earlay inway everyway adsporshay-ybay edpsoasongray awhay-osssmhay afsfolspray\n",
            "Epoch:  73 | Train loss: 0.238 | Val loss: 0.976 | Gen: eehay anghay away earlay inway everyway adsporhay-away-ayday epdosshpay-assingway awhaydossay ayfsay-onghay-away-a\n",
            "Epoch:  74 | Train loss: 0.246 | Val loss: 1.008 | Gen: erehay anghay away eardblay inway everbay ashordablyhay epdossshpay-aysday awhay-ashcay askgospray-oohengay\n",
            "Epoch:  75 | Train loss: 0.263 | Val loss: 1.098 | Gen: erehay anghay away earltay inway everyway adsporgay-osay-away- edposhspray-oseway awhossmay afsporknay\n",
            "Epoch:  76 | Train loss: 0.274 | Val loss: 0.873 | Gen: eerhay anghay away earllay inway everywray aposporksway epdosshpay-oathingra awhay-aysday akspongray\n",
            "Epoch:  77 | Train loss: 0.220 | Val loss: 0.807 | Gen: eerhay anghay away earllay inway everyway apdossporshay epdosspaschingway awhay-asossway akspongray\n",
            "Epoch:  78 | Train loss: 0.199 | Val loss: 0.771 | Gen: eerhay anghay away earllay inway everyway apdossporray epdostablishenchay awhay-oowshay aksporfay\n",
            "Epoch:  79 | Train loss: 0.190 | Val loss: 0.785 | Gen: eerhay anghay away earldpray inway everyway apdossproasdray epdosssatonhay awhousdway aksforspay-oompray\n",
            "Epoch:  80 | Train loss: 0.186 | Val loss: 0.803 | Gen: erehay anghay away earlgay inway everyway apdossporspray epdosspay-ookersgay awhay-udfay apponsdsfray\n",
            "Epoch:  81 | Train loss: 0.187 | Val loss: 0.878 | Gen: eerhay anghay away earldpray inway everyway apdosporshray-oasway epdoasshingspray awhasdway-ofway apponsfrasclay\n",
            "Epoch:  82 | Train loss: 0.190 | Val loss: 0.766 | Gen: eerhay anghay away earlay inway everyway apdossspray-oomray epdosshpay-oatingray awhay-oomsay askfuspray-oompay\n",
            "Epoch:  83 | Train loss: 0.182 | Val loss: 0.792 | Gen: erehay anghay away earlgay inway everyway adsporryhpay edpscoppray awhay-ofsway askfusprofgay\n",
            "Epoch:  84 | Train loss: 0.176 | Val loss: 0.778 | Gen: eerhay anghay away earllay inway everyway adsporray-asbay epdosshpastray awhossday apponsdsfray\n",
            "Epoch:  85 | Train loss: 0.169 | Val loss: 0.762 | Gen: eerhay anghay away earldpay inway everyway apdossproarshay epdosspasongray awhossday ay-omprosssay\n",
            "Epoch:  86 | Train loss: 0.163 | Val loss: 0.787 | Gen: eerhay anghay away earlpray inway everyway apdossprorsay epdoashspray awhossday askgorspay-onfay\n",
            "Epoch:  87 | Train loss: 0.164 | Val loss: 0.805 | Gen: eerhay anghay away earldpray inway everyway adsporryphpay epdosspasspray awhossday aksporfay\n",
            "Epoch:  88 | Train loss: 0.173 | Val loss: 1.026 | Gen: eerhay anghay away earllbay inway everyray apdsorsporsay epdoasshibnway awhossday askgospray-oofway\n",
            "Epoch:  89 | Train loss: 0.214 | Val loss: 1.015 | Gen: eerhay angsway away earlay inway everyway apporsdschay epdosspastpray awlay-ossay-aysday acksgsay-aysday\n",
            "Epoch:  90 | Train loss: 0.259 | Val loss: 1.093 | Gen: eerhay anghay away earlday inway everyway adsporfay-ashpray epfsackdpray awhaydosshay akfsospancray\n",
            "Epoch:  91 | Train loss: 0.236 | Val loss: 0.874 | Gen: erehay anghay away earlay inway everryway apdossproargay epdscoathinghay awhousdway ay-ooghmarnssway\n",
            "Epoch:  92 | Train loss: 0.212 | Val loss: 1.042 | Gen: eerhay anghay away earllay inway everyway askporspray epdshpoatsway-owmay- awhossway asgondsray-awayday\n",
            "Epoch:  93 | Train loss: 0.204 | Val loss: 0.779 | Gen: eerhay anghay away earlgay inway everyway apdossorspray epfolsshday awhay-aysday ay-okhhaghonshpray\n",
            "Epoch:  94 | Train loss: 0.165 | Val loss: 0.764 | Gen: eerhay anghay away earldpay inway everyway apdosssporray epdoashspray awhossway ay-okhhaghrongssay\n",
            "Epoch:  95 | Train loss: 0.153 | Val loss: 0.763 | Gen: eerhay anghay away earldpay inway everyway apdosssporsray epdschpowlay awhossway akfushprossfay\n",
            "Epoch:  96 | Train loss: 0.148 | Val loss: 0.789 | Gen: eerhay anghay away earlday inway everyway apdossprorshay epdschathingcray awhossway ay-okhhorngsshay\n",
            "Epoch:  97 | Train loss: 0.148 | Val loss: 0.762 | Gen: eerhay anghay away earldpay inway everyway apdosssproarday epdschpowray awhossway ay-ompronsshsnay\n",
            "Epoch:  98 | Train loss: 0.144 | Val loss: 0.782 | Gen: erehay anghay away earldpay inway everyway apdossproshray epdschpowray awhossway ay-ompronsssshingnay\n",
            "Epoch:  99 | Train loss: 0.142 | Val loss: 0.767 | Gen: eerhay anghay away earlday inway everyway apdosssporray epdschathingbray awhossway ay-ompronsshsnay\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA0-Q_3o5dLU",
        "outputId": "9d34b154-e5b5-4adb-baef-596992825a74"
      },
      "source": [
        "test_sentence = 'here hang a pearl in every'\r\n",
        "long_sentence=\"here hang a pearl in every cowslip ear\"\r\n",
        "translated_RNN_long = translate_sentence(test_sentence, rnn_encoder_long, rnn_decoder_long, None, args)\r\n",
        "long_translated_RNN_long = translate_sentence(long_sentence, rnn_encoder_long, rnn_decoder_long, None, args)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(test_sentence, translated_RNN_long))\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(long_sentence, long_translated_RNN_long))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\there hang a pearl in every \n",
            "translated:\teerhay anghay away earlday inway everyway\n",
            "source:\t\there hang a pearl in every cowslip ear \n",
            "translated:\teerhay anghay away earlday inway everyway owspithway earway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cuuWYodkxrA"
      },
      "source": [
        "## Attention Model Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08R2Ypv7kxrB"
      },
      "source": [
        "class RNNAttentionDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(RNNAttentionDecoder, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "\n",
        "        self.rnn = MyGRUCell(input_size=hidden_size*2, hidden_size=hidden_size)\n",
        "        self.attention = Attention(hidden_size=hidden_size)\n",
        "        \n",
        "        self.out = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, annotations, hidden_init):\n",
        "        \"\"\"Forward pass of the attention-based decoder RNN.\n",
        "\n",
        "        Arguments:\n",
        "            inputs: Input token indexes across a batch for all the time step. (batch_size x decoder_seq_len)\n",
        "            annotations: The encoder hidden states for each step of the input.\n",
        "                         sequence. (batch_size x seq_len x hidden_size)\n",
        "            hidden_init: The final hidden states from the encoder, across a batch. (batch_size x hidden_size)\n",
        "\n",
        "        Returns:\n",
        "            output: Un-normalized scores for each token in the vocabulary, across a batch for all the decoding time steps. (batch_size x decoder_seq_len x vocab_size)\n",
        "            attentions: The stacked attention weights applied to the encoder annotations (batch_size x encoder_seq_len x decoder_seq_len)\n",
        "        \"\"\"\n",
        "        \n",
        "        batch_size, seq_len = inputs.size()\n",
        "        embed = self.embedding(inputs)  # batch_size x seq_len x hidden_size        \n",
        "\n",
        "        hiddens = []\n",
        "        attentions = []\n",
        "        h_prev = hidden_init\n",
        "        for i in range(seq_len):\n",
        "            \n",
        "            embed_current = embed[:,i,:]\n",
        "            context, attention_weights = self.attention(embed_current, annotations, annotations)\n",
        "            embed_and_context = torch.cat((embed_current, context.squeeze(1)), 1)\n",
        "            h_prev = self.rnn(embed_and_context,h_prev)\n",
        "\n",
        "            \n",
        "            hiddens.append(h_prev)\n",
        "            attentions.append(attention_weights)\n",
        "\n",
        "        hiddens = torch.stack(hiddens, dim=1) # batch_size x seq_len x hidden_size\n",
        "        attentions = torch.cat(attentions, dim=2) # batch_size x seq_len x seq_len\n",
        "        \n",
        "        output = self.out(hiddens)  # batch_size x seq_len x vocab_size\n",
        "        return output, attentions    \n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.Q = nn.Linear(hidden_size, hidden_size)\n",
        "        self.K = nn.Linear(hidden_size, hidden_size)\n",
        "        self.V = nn.Linear(hidden_size, hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "        batch_size, seq_len, hidden_size = keys.size()\n",
        "        q = self.Q(queries.view(-1, hidden_size)).view(batch_size, -1, hidden_size) #batch_size x (k) x hidden_size\n",
        "        k = self.K(keys.view(-1,hidden_size)).view(batch_size, seq_len, hidden_size) #batch_size x seq_len x hidden_size\n",
        "        v = self.V(values.view(-1, hidden_size)).view(batch_size, seq_len, hidden_size) #batch_size x seq_len x hidden_size\n",
        "        unnormalized_attention = self.scaling_factor * torch.bmm(k, q.transpose(1,2)) #batch_size x seq_len x k\n",
        "        attention_weights = self.softmax(unnormalized_attention) #batch_size x seq_len x k\n",
        "        context = torch.bmm(attention_weights.transpose(1,2), v) #batch_size x k x hidden_size\n",
        "        return context, attention_weights"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZTuuLbgkxrC",
        "outputId": "50ab82b3-b365-44f9-9577-896ce6c0c6ca"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\") \n",
        "torch.manual_seed(1)\n",
        "args_attn = AttrDict()\n",
        "args_dict_attn = {'cuda':True,  'nepochs':100, 'checkpoint_dir':\"checkpoints\", 'learning_rate':0.005, 'lr_decay':0.99,'batch_size':64, 'hidden_size':20, 'decoder_type': 'rnn_attention' }\n",
        "args_attn.update(args_dict_attn)\n",
        "print_opts(args_attn)\n",
        "attention_rnn_encoder, attention_rnn_decoder = train(args_attn)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                                      Opts                                      \n",
            "--------------------------------------------------------------------------------\n",
            "                                   cuda: 1                                      \n",
            "                                nepochs: 100                                    \n",
            "                         checkpoint_dir: checkpoints                            \n",
            "                          learning_rate: 0.005                                  \n",
            "                               lr_decay: 0.99                                   \n",
            "                             batch_size: 64                                     \n",
            "                            hidden_size: 20                                     \n",
            "                           decoder_type: rnn_attention                          \n",
            "================================================================================\n",
            "================================================================================\n",
            "                                   Data Stats                                   \n",
            "--------------------------------------------------------------------------------\n",
            "('sober', 'obersay')\n",
            "('grey', 'eygray')\n",
            "('winter', 'interway')\n",
            "('downs', 'ownsday')\n",
            "('inforced', 'inforcedway')\n",
            "Num unique word pairs: 6387\n",
            "Vocabulary: dict_keys(['-', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'SOS', 'EOS'])\n",
            "Vocab size: 29\n",
            "================================================================================\n",
            "Moved models to GPU!\n",
            "Epoch:   0 | Train loss: 2.227 | Val loss: 1.957 | Gen: enesesesenesenenesen ay-ay-ay-ay-ay-ay-ay ay away inay-ay-ay-ay-ay-ay- esay\n",
            "Epoch:   1 | Train loss: 1.753 | Val loss: 1.748 | Gen: eray angay away aray-aray ingay-onay-onay-onay eray\n",
            "Epoch:   2 | Train loss: 1.536 | Val loss: 1.624 | Gen: ereray-eray-eray-era anghay away-ay away-ayday-ayday-ayd insay-y-y-y-y-y-y-y- eray-eray-eray-eray-\n",
            "Epoch:   3 | Train loss: 1.373 | Val loss: 1.520 | Gen: ererereray angay away away-ay-ay-ay-ay-ay insway eway\n",
            "Epoch:   4 | Train loss: 1.232 | Val loss: 1.431 | Gen: ererererereray angway away-ay-ay-ay-ay eartway inay-inay erssray\n",
            "Epoch:   5 | Train loss: 1.087 | Val loss: 1.259 | Gen: ereray angway ayay eay-ay-ay-ay-ay-ay-a inway ershay\n",
            "Epoch:   6 | Train loss: 0.944 | Val loss: 1.232 | Gen: ererereray anghgay away eaway-away inway ensiway\n",
            "Epoch:   7 | Train loss: 0.847 | Val loss: 1.079 | Gen: erereray angway ayay earay-aray-ay-ay insay-inay evenay\n",
            "Epoch:   8 | Train loss: 0.756 | Val loss: 0.962 | Gen: erereray angday away earlway inway ennay\n",
            "Epoch:   9 | Train loss: 0.671 | Val loss: 0.935 | Gen: ererereray angway ayay earlway inway-inway enterymay\n",
            "Epoch:  10 | Train loss: 0.659 | Val loss: 0.886 | Gen: erereray anghay away earlway-eearlway inway ernerymay\n",
            "Epoch:  11 | Train loss: 0.640 | Val loss: 0.988 | Gen: erereray angtnhay away earlspay inway eneyeneryay\n",
            "Epoch:  12 | Train loss: 0.595 | Val loss: 0.861 | Gen: ereray angthay away-ayayayayayayaya earlway-arlway inway-inway-inway eveveryay\n",
            "Epoch:  13 | Train loss: 0.549 | Val loss: 0.817 | Gen: ererereray angway away earlway-earlway inway-inway everyeryay\n",
            "Epoch:  14 | Train loss: 0.510 | Val loss: 0.774 | Gen: ererereray angway away earlway-earlway inway-inway everyesery\n",
            "Epoch:  15 | Train loss: 0.476 | Val loss: 0.730 | Gen: emereray angway away earlway-earlway inway-inway everyay\n",
            "Epoch:  16 | Train loss: 0.444 | Val loss: 0.811 | Gen: erererereray angway-elay away-ayayayayayayaya earlway-earlway inway-inway everyeserydryay\n",
            "Epoch:  17 | Train loss: 0.427 | Val loss: 0.674 | Gen: erereray angway away earlway-earlway inway-inway everyay\n",
            "Epoch:  18 | Train loss: 0.398 | Val loss: 0.660 | Gen: erereray anghaygay away earlway-earlway inway-inway everygryway\n",
            "Epoch:  19 | Train loss: 0.378 | Val loss: 0.610 | Gen: erereray angway away earlway-arlway inway-inway everyeshay\n",
            "Epoch:  20 | Train loss: 0.363 | Val loss: 0.635 | Gen: erereray anghayggay away earlway-arlway inway-inway everybyay\n",
            "Epoch:  21 | Train loss: 0.358 | Val loss: 0.674 | Gen: erereray anghay-anghay away earlway-arlway inway-inway everyesay\n",
            "Epoch:  22 | Train loss: 0.341 | Val loss: 0.684 | Gen: ererereray anghangway away earlway-earlway inway-inway everyryway\n",
            "Epoch:  23 | Train loss: 0.335 | Val loss: 0.674 | Gen: erereray angtngway away earlway-earlway inway-inway everyryryryryryryryr\n",
            "Epoch:  24 | Train loss: 0.347 | Val loss: 0.739 | Gen: erereray angway away earlway-earay inway everypymay\n",
            "Epoch:  25 | Train loss: 0.365 | Val loss: 0.616 | Gen: erereray anghay away earlway-earlway inway everyerypy\n",
            "Epoch:  26 | Train loss: 0.310 | Val loss: 0.554 | Gen: erereray anghay away earlway-earlway inway everyryryryay\n",
            "Epoch:  27 | Train loss: 0.289 | Val loss: 0.568 | Gen: erereray anghay away earlway inway everybryryryryryryry\n",
            "Epoch:  28 | Train loss: 0.296 | Val loss: 0.680 | Gen: erereray anghay away earlway-earlway inway everydray\n",
            "Epoch:  29 | Train loss: 0.315 | Val loss: 0.643 | Gen: ereray anghway away earlway inway everybay\n",
            "Epoch:  30 | Train loss: 0.299 | Val loss: 0.509 | Gen: erereray anghay away earlway-earlway inway everyway\n",
            "Epoch:  31 | Train loss: 0.272 | Val loss: 0.577 | Gen: erereray angway away earlway inway everyway\n",
            "Epoch:  32 | Train loss: 0.272 | Val loss: 0.520 | Gen: ereray anghay away earlway-earlway inway everyway\n",
            "Epoch:  33 | Train loss: 0.239 | Val loss: 0.485 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  34 | Train loss: 0.219 | Val loss: 0.461 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  35 | Train loss: 0.207 | Val loss: 0.459 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  36 | Train loss: 0.201 | Val loss: 0.472 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  37 | Train loss: 0.195 | Val loss: 0.479 | Gen: erereray anghay away earlway inway eveveryway\n",
            "Epoch:  38 | Train loss: 0.192 | Val loss: 0.473 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  39 | Train loss: 0.186 | Val loss: 0.482 | Gen: erereray anghay away earlway inway eveveveveryway\n",
            "Epoch:  40 | Train loss: 0.192 | Val loss: 0.472 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  41 | Train loss: 0.206 | Val loss: 0.680 | Gen: ererereray anghway away earlway inway evevevevevevevevevev\n",
            "Epoch:  42 | Train loss: 0.247 | Val loss: 0.582 | Gen: erereray angway away earlway-arlway inway-inway eveveveveryway\n",
            "Epoch:  43 | Train loss: 0.247 | Val loss: 0.626 | Gen: erereray anghay away earlway inway-inway eryweryway\n",
            "Epoch:  44 | Train loss: 0.229 | Val loss: 0.535 | Gen: ereray anghay away earlway inway eveveveneveryway\n",
            "Epoch:  45 | Train loss: 0.204 | Val loss: 0.477 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  46 | Train loss: 0.177 | Val loss: 0.410 | Gen: ereway anghay away earlway inway-inway eveveryway\n",
            "Epoch:  47 | Train loss: 0.171 | Val loss: 0.477 | Gen: ereray anglay away earlway inway eveveveveryway\n",
            "Epoch:  48 | Train loss: 0.185 | Val loss: 0.494 | Gen: erereray anghay away earlway inway-inway everyway\n",
            "Epoch:  49 | Train loss: 0.174 | Val loss: 0.422 | Gen: ereray anghay away earlway inway eveveryway\n",
            "Epoch:  50 | Train loss: 0.155 | Val loss: 0.389 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  51 | Train loss: 0.142 | Val loss: 0.386 | Gen: ereray anghay away earlway inway eveveryway\n",
            "Epoch:  52 | Train loss: 0.139 | Val loss: 0.394 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  53 | Train loss: 0.136 | Val loss: 0.398 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  54 | Train loss: 0.133 | Val loss: 0.396 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  55 | Train loss: 0.132 | Val loss: 0.413 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  56 | Train loss: 0.133 | Val loss: 0.454 | Gen: ereray anghay away earlway inway eveveveveryway\n",
            "Epoch:  57 | Train loss: 0.150 | Val loss: 0.458 | Gen: ereray anghay away earlway inway-inway everyway\n",
            "Epoch:  58 | Train loss: 0.146 | Val loss: 0.475 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  59 | Train loss: 0.142 | Val loss: 0.412 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  60 | Train loss: 0.131 | Val loss: 0.412 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  61 | Train loss: 0.128 | Val loss: 0.407 | Gen: erereway anghay away earlway inway everyway\n",
            "Epoch:  62 | Train loss: 0.125 | Val loss: 0.446 | Gen: ereray anghay away earlpay inway-inway everyway\n",
            "Epoch:  63 | Train loss: 0.139 | Val loss: 0.488 | Gen: ereray anghay away earlway inway-inway everyway\n",
            "Epoch:  64 | Train loss: 0.185 | Val loss: 0.665 | Gen: erereray angway away earlway inway-inway everybyway\n",
            "Epoch:  65 | Train loss: 0.230 | Val loss: 0.672 | Gen: ereray anghay away olway inway-inway everyray\n",
            "Epoch:  66 | Train loss: 0.185 | Val loss: 0.474 | Gen: ereray anghay away earlway inway-inway eveveryway\n",
            "Epoch:  67 | Train loss: 0.158 | Val loss: 0.428 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  68 | Train loss: 0.139 | Val loss: 0.393 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  69 | Train loss: 0.123 | Val loss: 0.378 | Gen: erereway anghay away earlway inway eveveveveryway\n",
            "Epoch:  70 | Train loss: 0.110 | Val loss: 0.368 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  71 | Train loss: 0.105 | Val loss: 0.366 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  72 | Train loss: 0.102 | Val loss: 0.368 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  73 | Train loss: 0.100 | Val loss: 0.369 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  74 | Train loss: 0.098 | Val loss: 0.370 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  75 | Train loss: 0.097 | Val loss: 0.371 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  76 | Train loss: 0.095 | Val loss: 0.375 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  77 | Train loss: 0.094 | Val loss: 0.373 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  78 | Train loss: 0.093 | Val loss: 0.381 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  79 | Train loss: 0.093 | Val loss: 0.378 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  80 | Train loss: 0.092 | Val loss: 0.391 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  81 | Train loss: 0.092 | Val loss: 0.396 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  82 | Train loss: 0.093 | Val loss: 0.393 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  83 | Train loss: 0.095 | Val loss: 0.415 | Gen: ereray anghay away earlway inway everyway\n",
            "Epoch:  84 | Train loss: 0.098 | Val loss: 0.411 | Gen: erereray anghay away earlpay inway everyway\n",
            "Epoch:  85 | Train loss: 0.098 | Val loss: 0.483 | Gen: ereray anghay away earlway inway everyray\n",
            "Epoch:  86 | Train loss: 0.122 | Val loss: 0.640 | Gen: erereray angngangay away earlpay inway-inway eveveveveryway\n",
            "Epoch:  87 | Train loss: 0.158 | Val loss: 0.614 | Gen: erereray anghay away earlway inway everyway\n",
            "Epoch:  88 | Train loss: 0.170 | Val loss: 0.608 | Gen: erereway anghay away earlway inway everyway\n",
            "Epoch:  89 | Train loss: 0.166 | Val loss: 0.443 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  90 | Train loss: 0.130 | Val loss: 0.389 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  91 | Train loss: 0.098 | Val loss: 0.354 | Gen: erereray anghay away earlpay inway everyway\n",
            "Epoch:  92 | Train loss: 0.091 | Val loss: 0.374 | Gen: erereray anghay away earlpay inway everyway\n",
            "Epoch:  93 | Train loss: 0.086 | Val loss: 0.377 | Gen: erereray anghay away earlpay inway everyway\n",
            "Epoch:  94 | Train loss: 0.082 | Val loss: 0.373 | Gen: erereray anghay away earlpay inway everyway\n",
            "Epoch:  95 | Train loss: 0.079 | Val loss: 0.371 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  96 | Train loss: 0.077 | Val loss: 0.376 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  97 | Train loss: 0.076 | Val loss: 0.377 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  98 | Train loss: 0.075 | Val loss: 0.383 | Gen: ereray anghay away earlpay inway everyway\n",
            "Epoch:  99 | Train loss: 0.074 | Val loss: 0.383 | Gen: ereray anghay away earlpay inway everyway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39meu8Ss7bfI",
        "outputId": "be99dc7f-401c-4b85-e5f5-aa27c41ce54d"
      },
      "source": [
        "test_sentence = 'here hang a pearl in every'\r\n",
        "long_sentence=\"here hang a pearl in every cowslip ear ever again hang in there for rest of the life\"\r\n",
        "translated_attn = translate_sentence(test_sentence, attention_rnn_encoder, attention_rnn_decoder, None, args)\r\n",
        "long_translated_attn = translate_sentence(long_sentence, attention_rnn_encoder, attention_rnn_decoder, None, args)\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(test_sentence, translated_attn))\r\n",
        "print(\"source:\\t\\t{} \\ntranslated:\\t{}\".format(long_sentence, long_translated_attn))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source:\t\there hang a pearl in every \n",
            "translated:\tereray anghay away earlpay inway everyway\n",
            "source:\t\there hang a pearl in every cowslip ear \n",
            "translated:\tereray anghay away earlpay inway everyway owslipcslay earway\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpjG7wNnypDt"
      },
      "source": [
        "#### Attention visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T6N4AR8zNxi"
      },
      "source": [
        "def visualize_attention(input_string, encoder, decoder, opts):\r\n",
        "    \"\"\"Generates a heatmap to show where attention is focused in each decoder step.\r\n",
        "    \"\"\"\r\n",
        "    line_pairs, vocab_size, idx_dict = load_data()\r\n",
        "    char_to_index = idx_dict['char_to_index']\r\n",
        "    index_to_char = idx_dict['index_to_char']\r\n",
        "    start_token = idx_dict['start_token']\r\n",
        "    end_token = idx_dict['end_token']\r\n",
        "\r\n",
        "    max_generated_chars = 20\r\n",
        "    gen_string = ''\r\n",
        "\r\n",
        "    indexes = string_to_index_list(input_string, char_to_index, end_token)\r\n",
        "    indexes = to_var(torch.LongTensor(indexes).unsqueeze(0), opts.cuda)  # Unsqueeze to make it like BS = 1\r\n",
        "\r\n",
        "    encoder_annotations, encoder_hidden = encoder(indexes)\r\n",
        "\r\n",
        "    decoder_hidden = encoder_hidden\r\n",
        "    decoder_input = to_var(torch.LongTensor([[start_token]]), opts.cuda)  # For BS = 1\r\n",
        "    decoder_inputs = decoder_input\r\n",
        "\r\n",
        "    produced_end_token = False\r\n",
        "\r\n",
        "    for i in range(max_generated_chars):\r\n",
        "     \r\n",
        "      decoder_outputs, attention_weights = decoder(decoder_inputs, encoder_annotations, decoder_hidden)\r\n",
        "      generated_words = F.softmax(decoder_outputs, dim=2).max(2)[1]\r\n",
        "      ni = generated_words.cpu().numpy().reshape(-1)  # LongTensor of size 1\r\n",
        "      ni = ni[-1] \r\n",
        "      \r\n",
        "      decoder_inputs = torch.cat([decoder_input, generated_words], dim=1)\r\n",
        "      \r\n",
        "      if ni == end_token:\r\n",
        "          break\r\n",
        "      else:\r\n",
        "          gen_string = \"\".join(\r\n",
        "              [index_to_char[int(item)] \r\n",
        "               for item in generated_words.cpu().numpy().reshape(-1)])\r\n",
        "    \r\n",
        "    if isinstance(attention_weights, tuple):\r\n",
        "      \r\n",
        "      attention_weights, self_attention_weights = attention_weights\r\n",
        "    \r\n",
        "    all_attention_weights = attention_weights.data.cpu().numpy()\r\n",
        "    \r\n",
        "    for i in range(len(all_attention_weights)):\r\n",
        "      attention_weights_matrix = all_attention_weights[i].squeeze()\r\n",
        "      fig = plt.figure()\r\n",
        "      ax = fig.add_subplot(111)\r\n",
        "      cax = ax.matshow(attention_weights_matrix, cmap='bone')\r\n",
        "      fig.colorbar(cax)\r\n",
        "\r\n",
        "      # Set up axes\r\n",
        "      ax.set_yticklabels([''] + list(input_string) + ['EOS'], rotation=90)\r\n",
        "      ax.set_xticklabels([''] + list(gen_string) + (['EOS'] if produced_end_token else []))\r\n",
        "\r\n",
        "      # Show label at every tick\r\n",
        "      ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "      ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\r\n",
        "      # Add title\r\n",
        "      plt.xlabel('Attention weights to the source sentence in layer {}'.format(i+1))\r\n",
        "      plt.tight_layout()\r\n",
        "      plt.grid('off')\r\n",
        "      plt.show()\r\n",
        "      plt.savefig('save.pdf')\r\n",
        "      plt.close(fig)\r\n",
        "\r\n",
        "    return gen_string\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6RguaGD2RcZ"
      },
      "source": [
        "words = 'roomba'"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "4QMayjFKz80e",
        "outputId": "56b99654-585f-4ee1-f93e-a484d9d5822c"
      },
      "source": [
        "visualize_attention(words, attention_rnn_encoder, attention_rnn_decoder,args_attn)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEhCAYAAABRKfYcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wddX3/8dc7mxBIAgkaCGEXkUoEU0EwXKSiRkQaawtatQK9iLWN/hRRYBNASEAQaJKt1gtqg6XRglJvYKr5GSyCotxyJTcIxiCyMVyCGCVALruf/jGzcDju7plzZjZnNnk/8ziPzMx+z2c+Z87u+ZyZ78x8FRGYmZnVa0izEzAzs8HJBcTMzBriAmJmZg1xATEzs4a4gJiZWUNcQMzMrCEuIJaLpJdLWtXsPMxs53MBMTOzhpS+gEg6T9Kq9PHx3T2X9Bv/A5LmSXpQ0g2STpb0c0m/kHRcE9IamuZxv6RvSxrRhBwAkHSzpCWSVkua2qQcpkk6J53+jKQfp9MnSbqhSTk1fbukeVxe+bcj6UpJH2tWPpZTRJT2AUwCVgIjgVHAauDo3TkX4OXADuAIki8AS4DrAAGnATc3IZ8AXp/OXwe0N/F35iXp/3sBq4CXNiGH1wHfSqfvAO4FhgGXAh/cXbdLxe/L0nR6CPDLZuXiR/5H2fdATgRuiogtEfE08F3gDc6FhyJiZUR0kxSyWyP5i1xJ8ge6sz0SET9Pp68n2VbNco6k+4C7gYOACU3IYQkwSdI+wFbgLuAYkt+XO5qQD5RjuxARvwKelHQ0cAqwLCKebEYult/QZidgDdlaMd1dMd9Nc97T6huqNeUGa5ImAycDJ0TEM5JuB/bc2XlExHZJDwFnAXcCK4A3A4cC9+/sfMqyXSp8hWTbHECyx2qDVNn3QO4A3iFphKSRwDtp3je4MuVSNi+TdEI6fSbwsyblMRp4Kv2QPJzkUFKz3AG0Az9Npz9E8m27GcW1TNsF4CZgCnAssLDJuVgOpS4gEbEUmEdyDPke4CsRsWx3z6WE1gIfkXQ/sC/wpSbl8UOSDv37gX8hOVzTLHcA44G7IuIx4Dma94WjTNuFiNgG3AZ8MyK6mpmL5aPmfCEys92VpCHAUuA9EfGLZudjjSv1HoiZ7VokTQTWkZz44eIxyHkPxMzMGuI9EDMza4gLiJmZNWTQFJBm3n6hmnP5Y2XJA5xLX5xL78qUy2AzaAoIUKY32bn8sbLkAc6lL86ld2XKZVAZTAXEzMxKZMDPwpKGxJAh+evU3nuP4g9/eDpXjO7uYq5ZGj16NJs3by4kVl5F5DJ06LDceYwcOZItW7bkjpNcIpA3l73YsuXZ3HGK+NsYOXIEW7Y8kzvO2HEH5I6x/blnGLZn/hslb3rs0dwxitourz7iT3PH2Pib3zD+wANzx1m+bNmmiNgvdyBgypQpsWnTpsztlyxZsjAiphSx7noM+H2ThgwZwogR++SOc+mlM5k58/JcMZ5++qnceQDMmDGD9vb2QmLlVUQu+47J/+F08SXTuPJTc3LHGT58r9wxpk8/h9mzP5c7zo6uHbljXHTRuVx99Wdyx3nfR6bljnHEIS9l5UP571v41c/lf5+L2i4/+fnPazeqYfHdd3PM6/Lf3WX0iBEP5w6S2rRpE4sXL87cXtLYotZdD99M0cyshAbDNXouIGZmJdTtAmJmZvUKvAdiZmYNCaI5w+rUxQXEzKxsArrLXz9cQMzMyiaAru7uZqdRkwuImVkJuQ/EzMwa4gJiZmZ1iwifxmtmZo3xHoiZmTXEp/GamVndAp/Ga2ZmDdolDmFJEtAWEY/shHzMzIzBcS+smoMvRFIGF+yEXMzMDCCCqOPRLFlH71kq6dgBzcTMzIAXbqZY9gKSaURCSQ8AhwIPA1sAkeycHNlH+6mk4wyPHj160qWXXpY70dbWA9mw4Te5YhQ1ImFbWxudnZ2FxMqriFyGDt0jdx7jx49j48bHcscZUsCIhOMO2J/HHn08d5wizoI54IBxPPpo/u3y0v3H5Y6x1/ChPLs1/yBZTz6e//UUtV2KGJFwy9NPM3LUqNxx3nLSSUsi4pjcgYDXHH10/PC22zK3P3DffQtbdz2ydqL/eT1BI2IuMBegpWVo5B1JEODyy8szImFHR0dpRiQsIpf9xh6UOw+PSNi7wkYkPKdMIxLmfz1FbZe161fljlHUiIRFGwyd6Jm+7kXEw709Bjo5M7PdU9T1LwtJUyStlbRO0oW9/Pxlkm6TtEzSCkl/UStm/uMFZmZWqEhv5571UYukFuAa4G3AROAMSROrml0CfDMijgZOB75YK64LiJlZCRXciX4csC4i1kfENuBG4LTqVQL7pNOjgZqdzr6Q0MyshOrsAxkraXHF/Ny0L7pHK1B5LV8ncHxVjMuAWyR9FBgJnFxrpS4gZmYlk9zKpK4CsqmAs7DOAOZFxL9KOgH4L0mvjog+R7ZyATEzK6GCz8LaAFSebtmWLqv0AWBKuu67JO0JjAX6PCfefSBmZmWTjgeS9ZHBImCCpEMk7UHSST6/qs2vgbcASHoVsCfwRH9BvQdiZlZCRe6BRMQOSWcDC4EW4LqIWC3pcmBxRMwHzgeulXQuyVG0s6JGEi4gZmYlE0BXwRcSRsQCqu5rGBEzK6bXAK+vJ6YLiJlZCQ2GK9FdQMzMSsgFxMzM6hbZO8ebygXEzKyEvAdiZmYNcQExM7O6NXAlelMMeAGZeMSrufmWW3LHWbdiBct/+UCuGIcd2Jo7j4RoaSlL7c2fy2+f2pg7ix07thcSpwjbtm9l46Prc8cZ2jIsd4wdO7bz1G/zb5effC//qNIvP+vUQuIU8XqK2i6Hvuyw3DEuvmQap//N+3LHKVoRA5oNtLJ8CpqZWYUst2lvNhcQM7OyafJY51m5gJiZlUzgTnQzM2uQO9HNzKwh3gMxM7OGuICYmVndfCsTMzNrmK8DMTOzhvg6EDMzq5tP4zUzs4a5gJiZWUMGQyf6kGYnYGZmVdJbmWR9ZCFpiqS1ktZJurCXn39G0vL08aCk39WK6T0QM7OSCaCru7uweJJagGuAtwKdwCJJ8yNizfPrjDi3ov1HgaNrxfUeiJlZCUUd/zI4DlgXEesjYhtwI3BaP+3PAL5RK2jmPRBJrwHekM7eERH3ZX2umZnVp84ukLGSFlfMz42IuRXzrcAjFfOdwPG9BZJ0MHAI8ONaK81UQCR9DPhn4LvpouslzY2Iz2d5vpmZZdfAiISbIuKYglZ/OvDtiOiq1VBZOmAkrQBOiIgt6fxI4K6IOLKP9lOBqQD7jxs36WvXX19H7r3b+uyzDN9rr1wxVt23InceAG1trXR2bigkVl5lyaUseUBxuUjKHaO1tZUNG/LnMmLE3rljvPSlY3jyyZr9ojU988wfcscoarsUMTLo+PHj2LjxsdxxPv7xc5YU9SE+YeLE+Levfz1z+788+uh+1y3pBOCyiPjzdP4igIi4upe2y4CPRMSdtdabdesLqKxGXemyXqW7TnMBjjjqqDj0yF7rTF3WrVhB3jhvn/K23HkAzJo1iwsuuKCQWHmVJZey5AHF5VLEkLZXXX0ln7jo4txxjj765Nwx3nfWqXx13vzccZYt+9/cMYraLvvsMzZ3jIsvmcaVn5qTO07RCj6NdxEwQdIhwAaSvYwzqxtJOhzYF7grS9CsBeQ/gXsk3ZTOvwP4j4zPNTOzOhR9JXpE7JB0NrAQaAGui4jVki4HFkdEzzeL04EbI+PKMxWQiPi0pNuBE9NF74+IZXW9AjMzy6zoK9EjYgGwoGrZzKr5y+qJmfkAYkQsBZbWE9zMzBozGK5E94WEZmalk/n6jqZyATEzK5mIuq8DaQoXEDOzEvIhLDMza4hv525mZnVr4Er0pnABMTMrIe+BmJlZ/eoY56OZXEDMzMrIBcTMzBoR3S4gZmbWgEGwA+ICYmZWNsmFhOWvIC4gZmYl5AICPHj/g5xywim545x77lQ+/MH2XDGuuvaG3HkAtO0zrLBYeRWRywX/+N4CMgm6unbkjjJ8j3yDhkEyEFQRY3kMHz4id4whGlJInHsXLajdqIZ3v+dNhcQZNXJM7hhFbZdnnv197hjd3V2FxCmWz8IyM7MGREB3V3ez06jJBcTMrIS8B2JmZo1xATEzs0YMgvrBkGYnYGZmVSKI7uyPLCRNkbRW0jpJF/bR5m8krZG0WtLXa8X0HoiZWQkV2QciqQW4Bngr0AkskjQ/ItZUtJkAXAS8PiKekrR/rbjeAzEzK5kgKSBZHxkcB6yLiPURsQ24ETitqs0/A9dExFMk63+8VlAXEDOzEqqzgIyVtLjiMbUqXCvwSMV8Z7qs0iuBV0r6uaS7JU2plaMPYZmZlVCdh7A2RcQxOVc5FJgATAbagJ9KOiIiftfXE7wHYmZWNhHQXcejtg3AQRXzbemySp3A/IjYHhEPAQ+SFJQ+uYCYmZVQwX0gi4AJkg6RtAdwOjC/qs3NJHsfSBpLckhrfX9BfQjLzKyEirwOJCJ2SDobWAi0ANdFxGpJlwOLI2J++rNTJK0BuoBpEfFkf3FdQMzMSqbnLKxCY0YsABZULZtZMR3AeekjExcQM7Oy8XggZmbWqF1qSFtJrwHekM7eERH3DUxKZma7u8ExHkims7AkfQy4Adg/fVwv6aMDmZiZ2e6s4LOwBoSyrFzSCuCEiNiSzo8E7oqII/toPxWYCjBmzJhJV1xxZe5Ex43bj8ceeyJXjP3Gj8+dB8AeLWJbVzm+HRSRS+ev+j1TL5O2tjY6Oztzx5Hyn1ne2trKhg3Vp7jXb0gBuRzYOp7fbNiYO05Xd/7RHot6j1qG5D/yXdR2CfL/Hba2HsiGDb/JHee8885dUsDFfAAc9CeHxvlXdWRuf+4Z7yxs3fXI+psgktO6enSly3oVEXOBuQDDh4+Iz3xmbsMJ9jj33KnkjfPBSz6ROw+Ag/cZxsO/315IrLyKyOWC9nxDBQN0dHTQXkCcIoa0verqK/nERRfnz6WAIVc/+ckZXHrpFbnjPL2lz4uBM5s9exbTp1+QO04RQ9oWtV2KKKxXXHEZM2ZcljtO4QbBIaysBeQ/gXsk3ZTOvwP4j4FJyczMovwj2mYrIBHxaUm3Ayemi94fEcsGLCszs93cYOhEz3wwMyKWAksHMBczMwOIoLu7/Lsgvg7EzKxkBuJK9IHgAmJmVjaxi11IaGZmO5H3QMzMrH6D40p0FxAzsxIaBPXDBcTMrIy8B2JmZnULd6KbmVmjvAdiZmYNcQExM7MGDI6zsPLfr9rMzIoVxY8HImmKpLWS1km6sJefnyXpCUnL08c/1YrpPRAzszIqsBNdUgtwDfBWoBNYJGl+RKypavrfEXF21rgDXkC2bXuW9evzj367dWv+OI888EjuPADGH9nGIw/kH5inCEXkUsQgTkXF2b5jW+4YEd2FxNnRlX/Ml67uLrY8szl3nKJurFdEnCJeT1Hbpasr/3gg3d1dbNmSP5ciJffCKjTkccC6iFgPIOlG4DSguoDUxYewzMxKqM5DWGMlLa54TK0K1wpUfoPuTJdVe5ekFZK+LemgWjn6EJaZWdnUP9b5pgKGtP0f4BsRsVXSB4GvAif19wTvgZiZlVB0R+ZHBhuAyj2KtnTZC+uLeDIitqazXwEm1QrqAmJmVkIFn4W1CJgg6RBJewCnA/MrG0gaXzF7KnB/raA+hGVmVjJFDygVETsknQ0sBFqA6yJitaTLgcURMR84R9KpwA7gt8BZteK6gJiZlc0AnIYVEQuABVXLZlZMXwRcVE9MFxAzs9IZHFeiu4CYmZVQd5cLiJmZ1St8M0UzM2tA0Z3oA8UFxMyshFxAzMysAZkvEGwqFxAzs7JxH4iZmTXMBcTMzBoxCOpHtgIi6RjgYuDg9DkCIiKOHMDczMx2S7vaWVg3ANOAlUAxI9uYmVnvgkHRia4sVU7SzyLixMxBk8FMpgKMHj160owZMxrPMNXW1kZnZ76R9/Y/oC13HgAj99qDLc/mH/GuCEXk8vij+UdXLOL9KUpxuaiAXFrp7NxQu2FN+T9MvF36yqWY7dLe3r6kgDE5ABg3/qA44x/Pz9z+s1edW9i665F1D+RSSV8BbgV67hdPRHy3t8YRMReYCyAp2tvb8+ZJR0cHeeOcPb0jdx4Axx3Zxr0ryvFhWUQu18yZnjuPOXNmM21a/jhS/g+n2bNnMX36BaXIZdasWVxwQf5curq6csfo6JhDe/u03HFaWlpyxyhuu+Qf0raIz5aBsCsdwno/cDgwjBcOYQXQawExM7N8dqUCcmxEHDagmZiZ2QsGQQHJOiLhnZImDmgmZmYGJLWj4CFtB0TWPZDXAcslPUTSB+LTeM3MBtAg2AHJXECmDGgWZmZWYRcaUCoiHh7oRMzM7AWDoYBk7QMxM7OdJb2ZYtZHFpKmSForaZ2kC/tp9y5Jkd6BpF++F5aZWckExV6JLqkFuAZ4K9AJLJI0PyLWVLXbG/gYcE+WuN4DMTMroYL3QI4D1kXE+ojYBtwInNZLuyuAWcBzWYK6gJiZlU0E0d2d+QGMlbS44jG1KmIr8EjFfGe67HmSXgscFBE/yJqmD2GZmZVQnX3om/LcC0vSEODTwFn1PM8FxMyshAq+QHADcFDFfFu6rMfewKuB29N7wB0AzJd0akQs7iuoC4iZWckMwHggi4AJkg4hKRynA2c+v76IzcDYnnlJtwPt/RUPcAExMyufgsdEj4gdks4GFgItwHURsVrS5cDiiJjfSFwXEDOz0in+SvSIWAAsqFo2s4+2k7PE3K0KyBdmF3PP/46OjsJi5VVELkX8ot5+++10d+cfs6KIMTiAQnLZb+xBtRvVMHToMF6y7/jccfYYvmfuGMOGDae19dDccbZtzXSGZ7+K2i5PbHqkdqNBajBcib5bFRAzs8FiMAxp6wJiZlY2SS96s7OoyQXEzKxkBkn9cAExMysj94GYmVkDdqHxQMzMbCcKd6KbmVmDvAdiZmZ1G4BbmQwIFxAzsxJyATEzswbEoDiP1wXEzKxsAqK72UnU5gJiZlZCPoRlZmYNcQExM7O6+SwsMzNrTMEDSg2UTAVE0p7Ah4ETSYrjz4AvRUT+gQHMzKxKEF3l70VXlion6ZvAH4Dr00VnAmMi4j19tJ8KTAUYPXr0pBkzZuROtK2tjc7OztxxirCr5TJp0qTceTz99NOMGjUqd5wlS5bkjlHU+zN06B65Y4wfP46NGx/LHaeIgbYOOGAcjz6aP5civhkXtV127NiWO0ZRvy/t7e1LIuKY3IGAffcdF5Mnn5G5/c03f7awddcjawFZExETay3r47mF7Id1dHTQ3l6eUQB3pVyKGpFw8uTJueMU8UFZ1PtTxIiEF18yjSs/NSd3nCJGJJw+/Rxmz/5c7jhFjEhY1HYpYkTCAv+eC/sQHzNmXEyefHrm9t/73ueaUkCGZGy3VNLremYkHQ8sHpiUzMx2d0FEd+ZHFpKmSForaZ2kC3v5+YckrZS0XNLPJNXcQei3D0TSSpI+j2HAnZJ+nc4fDDyQKWszM6tbkZ3oklqAa4C3Ap3AIknzI2JNRbOvR8SX0/anAp8GpvQXt1Yn+l82nrKZmTWq4LOwjgPWRcR6AEk3AqcBzxeQiPh9RfuRJDsL/eq3gETEww2lamZmudRZQMZKquxWmBsRcyvmW4HKDqNO4PjqIJI+ApwH7AGcVGulvg7EzKxkIiJz30ZqUxGd6BFxDXCNpDOBS4D39dc+aye6mZntTBHZH7VtACpPK2xLl/XlRuAdtYK6gJiZlVDU8S+DRcAESYdI2gM4HZhf2UDShIrZtwO/qBXUh7DMzEqoyE70iNgh6WxgIdACXBcRqyVdDiyOiPnA2ZJOBrYDT1Hj8BW4gJiZlVLR98KKiAXAgqplMyumP1ZvTBcQM7PSqbsTvSlcQMzMSiZ2pbvxmpnZzuUCYmZmDXEBMTOzBmS+vqOpXEDMzEoocCe6mZk1wIewABBDhw7LH0XKPUJcV9eO3Hn0kMpzEX/eXIp4LR0dc3jzm2vee62mxzdvzh3jvsWLC4nTtt+43DG6uraz+fdP5I7zilccnTtGS8tQ9tlnbO44v/zlstwxitouRYwaWcRnCxQzOmIPn4VlZmYNChcQMzNrTHd3V7NTqMkFxMyshLwHYmZm9ct+m/amcgExMyuZgKy3aW8qFxAzsxLyzRTNzKwBPgvLzMwa5AJiZmYNcQExM7O6JSdhlb8PpDz34zAzs1TSB5L1kYWkKZLWSlon6cJefn6epDWSVki6VdLBtWK6gJiZlVHPtSBZHjVIagGuAd4GTATOkDSxqtky4JiIOBL4NjC7VlwXEDOzEoo6/mVwHLAuItZHxDbgRuC0F60v4raIeCadvRtoqxXUfSBmZiVUZyf6WEmLK+bnRsTcivlW4JGK+U7g+H7ifQD4/7VWmrmASNoXmADs2bMsIn6a9flmZpZV1NuJvikijilizZL+DjgGeFOttpkKiKR/Aj5GskuzHHgdcBeQfwAIMzN7kQEYD2QDcFDFfFu67EUknQxcDLwpIrbWCqosSUpaCRwL3B0RR0k6HLgqIv66j/ZTgakAo0ePnjRz5sya66iltbWVDRv+6PXWpag3pK2tjc7OzkJi5VWWXIrK4zVH5x846dktW9hr5MjccVYsX547RhG/twDDh4/IHWP//V/K448/mTvO1q3P1G5UQ1HbpQhF5XL++ecvKWovYMSIfeKww47L3H758lv7XbekocCDwFtICsci4MyIWF3R5miSzvMpEfGLLOvNegjruYh4ThKShkfEA5IO66txeuxtbpLUkLjggosyrqZvs2ZdTd44RY1IOGfObKZNm15IrLyKyKWIwtrRMYf29mm54zy++Xe5Y9y3eDGvOSb/3/Hb3/b23DGuvvpKLrro4txxihiR8CMf+QeuueZrueMUMSJhUduluzv/tRJFfLYMhCL3QCJih6SzgYVAC3BdRKyWdDmwOCLmA3OAUcC3JAH8OiJO7S9u1gLSKWkMcDPwI0lPAQ83+FrMzKyGoq9Ej4gFwIKqZTMrpk+uN2amAhIR70wnL5N0GzAa+GG9KzMzsywCBsGV6HWfxhsRPxmIRMzM7AUeD8TMzOo2AGdhDQgXEDOz0gm6u7uanURNLiBmZiXkPRAzM2uIC4iZmdXNfSBmZtagbLdpbzYXEDOzEgp2wetAzMxs4PkQlpmZNcQFxMzMGpB9rPNmcgExMyuZ5Cws94GYmVkDvAdiZmYNcQEBINixY1v+KFFMnKKUafcyfy4qJI8i7D96dO4YHR0dvPUtb8kd5wfL8g+ctGPjRm66567ccf5qUv4Bsp577l2sXXtv7jj/s2Rx7hhFbZe3FzCCZdk+WxK+DsTMzBrk27mbmVlDynSUoy8uIGZmJTNY7oU1pNkJmJlZteQ6kKyPLCRNkbRW0jpJF/by8zdKWipph6R3Z4npAmJmVkJFFhBJLcA1wNuAicAZkiZWNfs1cBbw9aw5+hCWmVkJFXwI6zhgXUSsB5B0I3AasKZifb9Kf5a588UFxMyshOrsRB8rqfL86rkRMbdivhV4pGK+Ezg+R3qAC4iZWflE3deBbIqI/BcL1ckFxMysZILCrwPZABxUMd+WLsvFBcTMrIS6u7uKDLcImCDpEJLCcTpwZt6gPgvLzKx0ij2NNyJ2AGcDC4H7gW9GxGpJl0s6FUDSsZI6gfcA/y5pda243gMxMyuhoi8kjIgFwIKqZTMrpheRHNrKzAXEzKxkBsuV6C4gZmYl5AJiZmYNCPDNFM3MrBGD4Xbu6m83SdKxwCMR8Wg6/w/Au4CHgcsi4rd9PG8qMBVg9OjRk2bMmJE70ba2Njo7O3PHKYJzKW8eUFwuEyZW3yqofrF9Oxo2LHecX6xZU7tRDd4uvStqu7S3ty8p6mK+oUOHxd57vyRz+9/97vHC1l2PWgVkKXByRPxW0huBG4GPAkcBr4qImndslFRIGe3o6KC9vb2IULnternkH5Gwo2MO7e3TcsehgG9dRb0/RY1IOHT8+NxxihiRcPbsWUyffkHuOEWNSFjEdiliRMIC/54LLSCjRu2buf3mzU80pYDUOoTVUrGX8V6S+6t8B/iOpOUDm5qZ2e4pub6j/H0gtS4kbJHUU2TeAvy44mfuPzEzGyBFjwcyEGoVgW8AP5G0CXgWuANA0qHA5gHOzcxstzXoT+ONiCsl3QqMB26JF17REJK+EDMzGwCDvoAARMTdkt4MvF8SwOqIuG3AMzMz250N9gIiqRX4LvAcsCRd/B5Js4B3RkTu2wGbmVm1ICh/J3qtPZAvAF+KiHmVC9PrQb5IMiSimZkVaLDcC6vWWVgTq4sHQER8DTh8QDIyM7Nd4iysXguMpCFAS/HpmJkZ7Bp7IN+XdK2kkT0L0ukvU3VfeTMzK0qxA0oNlFoFZDrJ9R4PS1oiaQnwK+D3QDnu5WFmtguK6M78aJZa14FsB9olzQAOTRf/MiKeGfDMzMx2U7tEJ7qk6QAR8SxweESs7Ckekq7aCfmZme2GYlDsgdQ6hHV6xfRFVT+bUnAuZmaWGgwFpNZZWOpjurd5MzMryGA4hFWrgEQf073Nm5lZQQZDAak1oFQXsIVkb2MvoKfzXMCeEVFzSDFJT5CMYJjXWGBTAXGK4Fz+WFnyAOfSF+fSu6JyOTgi9isgDpJ+SJJXVpsiYqd3K/RbQMpE0uJmjLjVG+dS3jzAufTFufSuTLkMNrU60c3MzHrlAmJmZg0ZTAVkbrMTqOBc/lhZ8gDn0hfn0rsy5TKoDJoCEhF1v8mS3iEpJB1esewoSX9RMT9Z0p81moukMZI+XDF/oKRv15tro3rbLpI+lN5yv0+SzpL0hT5+9ol6cpB0FvD9ep8j6cCK+V9JqqfTsE+N/K4MlDy5SHq5pDOblYukO+tsP0/Suwcilz7Wd7ikuyRtldTwrZXK9Psy2AyaAtKgM4Cfpf/3OAr4i4r5yUBdBaTKGOD5AhIRv4mITH9EAyUivpzecr9RdRUQ4CzgwFqNCnjOTiOp5midO8HLgcIKSL0iIs/fReF6eU9+C5wDdDQhHYP67jk/mB7AKGAD8EpgbbpsD+DXwBPAcuAC4IHp5hEAAAeFSURBVNG03XLgDcB+wHeARenj9elzLwOuA24H1gPnpMtvBJ5Nnz+H5I9+VfqzPYH/BFYCy4A3p8vPIhnp8YfAL4DZveR/LPDddPq0dB17pDHXp8tfkcZYAtxBcruZnlzbK+KsqMhvVX85AP8CdKXtbwBGAj8A7gNWAe+tyvPdwNPA2vQ5ewFvSV/vynSbDc/wnF8BnwSWps/reS0j0xj3pjFP62VbjQd+msZaBbwhXX5GGmsVMKui/dNVucxLp+eR3Gn6HuDTJPd/+9/0tS8FXpG2m0byu7EC+GQv+bSksVal6z+3xvs1D/gccCfJ79a70+V3k9zMdDlwbhp3TsW6P5i2m0zye/lt4IH0fVPF+39n+hruBfbuK04vr+PpWvGr2s+ryH1mGn8VySEipa9/aUX7CT3zwCTgJ+m2WQiMT5ffDvwbsBg4v488LyP9ffdjJ3/ONjuBAXth8LfAf6TTdwKT0umzgC9UtHvRLx/wdeDEdPplwP0V7e4EhpOcn/0kMIyKgpG2e34eOB+4Lp0+nKR47ZnmsB4Ync4/DBxUlf9QXigUHekf4+uBNwHfSJffCkxIp48Hflz9mtI/4BPS6X/hxQWk1xx48Qfsu4BrK+ZH97KtbweOSaf3BB4BXpnOfw34eH/PSed/BXw0nf4w8JV0+irg79LpMcCDwMiqWOcDF6fTLSQfkgem23u/dFv+GHhHL6+vuoB8H2hJ5+8hGbq553WNAE7hhQ/EIWn7N1blMwn4UcX8mBrv1zzgW2m8icC6dPlk4PsVcaYCl6TTw0k+VA9J220G2tIYdwEnknzhWA8cmz5nn3Rb9Bqnl/eosoD8Ufxe2s/jhQLykorl/wX8VTp9G3BUxXv7UZK/ozuB/dLl7+WFv5vbgS/W+Fu/DBeQpjzKsJs+UM4APptO35jOL+m7+fNOBiZKz9+pZR9Jo9LpH0TEVmCrpMeBcTVinQh8HiAiHpD0MMkeEcCtEbEZQNIa4GCSD17S9jsk/VLSq4DjSL4Rv5HkA/KONKc/A75VkevwypVLGgPsHRF3pYu+DvxlRZN+c0itBP5V0iySD7M7arzmw4CHIuLBdP6rwEdIvkXW8t30/yXAX6fTpwCnVhzj3pO0sFc8bxFwnaRhwM0RsVzSScDtEfFE+vpuINl+N9fI4VsR0SVpb6A1Im4CiIjn0jinpDktS9uPIvkm/dOKGOuBP5H0eZK9t1syvF83R3JTozWS+vq9OgU4sqKfYXS67m3AvRHRmea4nOSLzGZgY0QsSl/D7yteQ29xHupnu/QW/2f9tH9zejPWEcBLgNXA/wBfAd4v6TySQnEcye/Mq4EfpdumBdhYEeu/+1mPNdEuWUAkvQQ4CThCUpD8QoakaRmePgR4Xc8HRkVMgK0Vi7rIt/2yxPop8DZgO8mhlHkkr2VamufvIuKogcwhIh6U9FqSfqNPSbo1Ii7Psc4s+VTmIuBdEbG2rydFxE8lvRF4OzBP0qdJPjz7fErF9J5VP9tSI0cBV0fEv/eTz1OSXgP8OfAh4G+Aj9P/+1X5XvR1nzmR7KUtfNFCaTL1/W72GqeGzPEl7Ql8kWQP8xFJl/HCdv4OcCnJHuGSiHgyPZlidUSc0EfIWu+JNcmu2on+buC/IuLgiHh5RBxE8u3qDcAfSA5x9Kiev4VktxpIztqqsa7q51e6g+RQGpJeSfLNuc8Pwj6e/3HgrvSb9EtJvq2tSr9NPiTpPWl8pR9az4uI3wF/kHR8uqjy7sr92Z5+myf9434mIq4nOW7+2l7aV26DtcDLJfWMH/P3JMe2+3tOfxYCH1VawSUdXd1A0sHAYxFxLck33NeSHO9/k6SxklpI9kB78nhM0qvSoZnf2dtKI+IPQKekd6TrGC5pRJrPP/bslUpqlbR/VT5jgSER8R3gEuC1Wd6vXlRvo4XA/6t4b16pitFCe7EWGC/p2LT93mlHdL1x6tVTLDal2+n5k0rSL2YLgS+R9A/25LmfpBPSfIZJ+tMC87EBsqsWkDOAm6qWfSddfhvJIarlkt5Lslv9znT+DSRndRwjaUV6WOdD/a0oIp4Efi5plaQ5VT/+IjBE0kqS3fCz0kNgWd1Dcpis5/DICmBlRPR8g/5b4AOS7iM5RHBaLzE+AFybHnYYSf/fzHvMBVakh32OAO5Nn38p8Kle2s8Dvpy2EfB+kkM1K4Fuko7pPp8jaa9+crmC5Bj5Ckmr0/lqk4H7JC0jOSzy2YjYCFxI8n7fR/Jt93tp+wtJ+i7u5MWHSqr9PXCOpBVp2wMi4haSQ4F3pa/v2/xxIWwFbk+3x/W8MBRClver0gqgS9J9ks4lKY5rgKWSVgH/Tj97AhGxLd0en0/X+SOSD/e64tQr/eJyLUn/20KSQ4yVbiD5vbilIs93A7PSPJeT4cxISQdI6gTOAy6R1Clpn6Jeh9U2aO6FZY2RNCoink6nLyQ5u+VjTU7LdmNpf9boiJjR7Fwsn12yD8Re5O2SLiJ5rx8mOfvKrCkk3URyOu9Jzc7F8vMeiJmZNWRX7QMxM7MB5gJiZmYNcQExM7OGuICYmVlDXEDMzKwh/wcniljeRTGDEAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'oombaway'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}